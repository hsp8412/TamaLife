This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-16T13:51:08.626Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.expo/
  devices.json
  README.md
  settings.json
app/
  app/
    (tabs)/
      _layout.tsx
      explore.tsx
      food.tsx
      index.tsx
    _layout.tsx
    +not-found.tsx
    protectedRoutes.tsx
    sign-in.tsx
  components/
    __tests__/
      __snapshots__/
        ThemedText-test.tsx.snap
      ThemedText-test.tsx
    ui/
      IconSymbol.ios.tsx
      IconSymbol.tsx
      TabBarBackground.ios.tsx
      TabBarBackground.tsx
    Collapsible.tsx
    ExternalLink.tsx
    HapticTab.tsx
    HelloWave.tsx
    ParallaxScrollView.tsx
    ThemedText.tsx
    ThemedView.tsx
  constants/
    Colors.ts
  contexts/
    authContext.tsx
  hooks/
    useColorScheme.ts
    useColorScheme.web.ts
    useThemeColor.ts
  scripts/
    reset-project.js
  services/
    authService.ts
    httpService.ts
    toastService.ts
  types/
    user.ts
  utils/
    bundleResourceIO.ts
  .gitignore
  app.json
  metro.config.js
  package.json
  README.md
  tsconfig.json
backend/
  middleware/
    auth.ts
  ml/
    src/
      api.py
      convert_to_tflite.py
      dataset.py
      model.py
      organize_dataset.py
      repomix-output.txt
      test_api.py
      test_model_webcam.py
      test_model.py
      train.py
    requirements.txt
  models/
    user.ts
  repos/
    foodRepo.ts
    stateRepo.ts
    userRepo.ts
  routes/
    auth.ts
    food.ts
    ml.ts
    pet.ts
    state.ts
  services/
    llmService.ts
  startup/
    db.ts
    routes.ts
  tests/
    llm-pet-test.js
    llm-test.ts
  .gitignore
  index.ts
  package.json
  repomix-output.txt
.gitignore
.repomixignore
babel.config.js
package.json
README.md
setup.sh
tsconfig.json

================================================================
Files
================================================================

================
File: .expo/devices.json
================
{
  "devices": []
}

================
File: .expo/README.md
================
> Why do I have a folder named ".expo" in my project?

The ".expo" folder is created when an Expo project is started using "expo start" command.

> What do the files contain?

- "devices.json": contains information about devices that have recently opened this project. This is used to populate the "Development sessions" list in your development builds.
- "packager-info.json": contains port numbers and process PIDs that are used to serve the application to the mobile device/simulator.
- "settings.json": contains the server configuration that is used to serve the application manifest.

> Should I commit the ".expo" folder?

No, you should not share the ".expo" folder. It does not contain any information that is relevant for other developers working on the project, it is specific to your machine.

Upon project creation, the ".expo" folder is already added to your ".gitignore" file.

================
File: .expo/settings.json
================
{
  "hostType": "lan",
  "lanType": "ip",
  "dev": true,
  "minify": false,
  "urlRandomness": "KZK_jnY",
  "https": false
}

================
File: app/app/(tabs)/_layout.tsx
================
import { Tabs } from "expo-router";
import React from "react";
import { Platform } from "react-native";

import { HapticTab } from "@/components/HapticTab";
import { IconSymbol } from "@/components/ui/IconSymbol";
import TabBarBackground from "@/components/ui/TabBarBackground";
import { Colors } from "@/constants/Colors";
import { useColorScheme } from "@/hooks/useColorScheme";

export default function TabLayout() {
  const colorScheme = useColorScheme();

  return (
    <Tabs
      screenOptions={{
        tabBarActiveTintColor: Colors[colorScheme ?? "light"].tint,
        headerShown: false,
        tabBarButton: HapticTab,
        tabBarBackground: TabBarBackground,
        tabBarStyle: Platform.select({
          ios: {
            // Use a transparent background on iOS to show the blur effect
            position: "absolute",
          },
          default: {},
        }),
      }}
    >
      <Tabs.Screen
        name="index"
        options={{
          title: "Home",
          tabBarIcon: ({ color }) => (
            <IconSymbol size={28} name="house.fill" color={color} />
          ),
        }}
      />
      <Tabs.Screen
        name="explore"
        options={{
          title: "Explore",
          tabBarIcon: ({ color }) => (
            <IconSymbol size={28} name="paperplane.fill" color={color} />
          ),
        }}
      />
      <Tabs.Screen
        name="food"
        options={{
          title: "Food",
          tabBarIcon: ({ color }) => (
            <IconSymbol size={28} name="paperplane.fill" color={color} />
          ),
        }}
      />
    </Tabs>
  );
}

================
File: app/app/(tabs)/explore.tsx
================
import { StyleSheet, Image, Platform } from 'react-native';

import { Collapsible } from '@/components/Collapsible';
import { ExternalLink } from '@/components/ExternalLink';
import ParallaxScrollView from '@/components/ParallaxScrollView';
import { ThemedText } from '@/components/ThemedText';
import { ThemedView } from '@/components/ThemedView';
import { IconSymbol } from '@/components/ui/IconSymbol';

export default function TabTwoScreen() {
  return (
    <ParallaxScrollView
      headerBackgroundColor={{ light: '#D0D0D0', dark: '#353636' }}
      headerImage={
        <IconSymbol
          size={310}
          color="#808080"
          name="chevron.left.forwardslash.chevron.right"
          style={styles.headerImage}
        />
      }>
      <ThemedView style={styles.titleContainer}>
        <ThemedText type="title">Explore</ThemedText>
      </ThemedView>
      <ThemedText>This app includes example code to help you get started.</ThemedText>
      <Collapsible title="File-based routing">
        <ThemedText>
          This app has two screens:{' '}
          <ThemedText type="defaultSemiBold">app/(tabs)/index.tsx</ThemedText> and{' '}
          <ThemedText type="defaultSemiBold">app/(tabs)/explore.tsx</ThemedText>
        </ThemedText>
        <ThemedText>
          The layout file in <ThemedText type="defaultSemiBold">app/(tabs)/_layout.tsx</ThemedText>{' '}
          sets up the tab navigator.
        </ThemedText>
        <ExternalLink href="https://docs.expo.dev/router/introduction">
          <ThemedText type="link">Learn more</ThemedText>
        </ExternalLink>
      </Collapsible>
      <Collapsible title="Android, iOS, and web support">
        <ThemedText>
          You can open this project on Android, iOS, and the web. To open the web version, press{' '}
          <ThemedText type="defaultSemiBold">w</ThemedText> in the terminal running this project.
        </ThemedText>
      </Collapsible>
      <Collapsible title="Images">
        <ThemedText>
          For static images, you can use the <ThemedText type="defaultSemiBold">@2x</ThemedText> and{' '}
          <ThemedText type="defaultSemiBold">@3x</ThemedText> suffixes to provide files for
          different screen densities
        </ThemedText>
        <Image source={require('@/assets/images/react-logo.png')} style={{ alignSelf: 'center' }} />
        <ExternalLink href="https://reactnative.dev/docs/images">
          <ThemedText type="link">Learn more</ThemedText>
        </ExternalLink>
      </Collapsible>
      <Collapsible title="Custom fonts">
        <ThemedText>
          Open <ThemedText type="defaultSemiBold">app/_layout.tsx</ThemedText> to see how to load{' '}
          <ThemedText style={{ fontFamily: 'SpaceMono' }}>
            custom fonts such as this one.
          </ThemedText>
        </ThemedText>
        <ExternalLink href="https://docs.expo.dev/versions/latest/sdk/font">
          <ThemedText type="link">Learn more</ThemedText>
        </ExternalLink>
      </Collapsible>
      <Collapsible title="Light and dark mode components">
        <ThemedText>
          This template has light and dark mode support. The{' '}
          <ThemedText type="defaultSemiBold">useColorScheme()</ThemedText> hook lets you inspect
          what the user's current color scheme is, and so you can adjust UI colors accordingly.
        </ThemedText>
        <ExternalLink href="https://docs.expo.dev/develop/user-interface/color-themes/">
          <ThemedText type="link">Learn more</ThemedText>
        </ExternalLink>
      </Collapsible>
      <Collapsible title="Animations">
        <ThemedText>
          This template includes an example of an animated component. The{' '}
          <ThemedText type="defaultSemiBold">components/HelloWave.tsx</ThemedText> component uses
          the powerful <ThemedText type="defaultSemiBold">react-native-reanimated</ThemedText>{' '}
          library to create a waving hand animation.
        </ThemedText>
        {Platform.select({
          ios: (
            <ThemedText>
              The <ThemedText type="defaultSemiBold">components/ParallaxScrollView.tsx</ThemedText>{' '}
              component provides a parallax effect for the header image.
            </ThemedText>
          ),
        })}
      </Collapsible>
    </ParallaxScrollView>
  );
}

const styles = StyleSheet.create({
  headerImage: {
    color: '#808080',
    bottom: -90,
    left: -35,
    position: 'absolute',
  },
  titleContainer: {
    flexDirection: 'row',
    gap: 8,
  },
});

================
File: app/app/(tabs)/food.tsx
================
// app/(tabs)/food.tsx
import React, { useState } from "react";
import {
  View,
  Text,
  Button,
  Image,
  StyleSheet,
  Alert,
  ActivityIndicator,
} from "react-native";
import * as ImagePicker from "expo-image-picker";
import { MaterialIcons } from "@expo/vector-icons";

const CATEGORIES = ["non_food", "food", "junk_food"];
const API_URL = process.env.EXPO_PUBLIC_API_URL;

const Food = () => {
  const [image, setImage] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [prediction, setPrediction] = useState<string | null>(null);
  const [confidence, setConfidence] = useState<number | null>(null);

  const pickImage = async () => {
    try {
      const { status } =
        await ImagePicker.requestMediaLibraryPermissionsAsync();
      if (status !== "granted") {
        Alert.alert(
          "Sorry, we need camera roll permissions to make this work!"
        );
        return;
      }

      const result = await ImagePicker.launchImageLibraryAsync({
        mediaTypes: ImagePicker.MediaTypeOptions.Images,
        allowsEditing: true,
        aspect: [1, 1],
        quality: 1,
      });

      if (!result.canceled) {
        setImage(result.assets[0].uri);
        await processImage(result.assets[0].uri);
      }
    } catch (error) {
      console.error("Error picking image:", error);
      Alert.alert("Error", "Failed to pick image");
    }
  };

  const processImage = async (uri: string) => {
    try {
      setIsLoading(true);
      const endpoint = `${API_URL}ml/predict`;
      console.log("Sending request to:", endpoint);

      // Create form data
      const formData = new FormData();
      formData.append("image", {
        uri: uri,
        type: "image/jpeg",
        name: "image.jpg",
      } as any);

      // Log the request details
      console.log("Request details:", {
        url: endpoint,
        method: "POST",
        headers: {
          Accept: "application/json",
          "Content-Type": "multipart/form-data",
        },
        body: formData,
      });

      // Send to backend for processing
      const response = await fetch(endpoint, {
        method: "POST",
        body: formData,
        headers: {
          Accept: "application/json",
          "Content-Type": "multipart/form-data",
        },
      });

      // Log the response status
      console.log("Response status:", response.status);

      if (!response.ok) {
        const errorText = await response.text();
        console.error("Error response:", errorText);
        throw new Error(
          `HTTP error! status: ${response.status}, message: ${errorText}`
        );
      }

      const result = await response.json();
      console.log("Prediction result:", result);
      setPrediction(result.category);
      setConfidence(result.confidence);
    } catch (error) {
      console.error("Error processing image:", error);
      Alert.alert("Error", `Failed to process image: ${error.message}`);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <View style={styles.container}>
      <Text style={styles.title}>Food Detector</Text>

      <Button title="Pick an image" onPress={pickImage} />

      {isLoading && (
        <View style={styles.loadingContainer}>
          <ActivityIndicator size="large" color="#0000ff" />
          <Text>Processing image...</Text>
        </View>
      )}

      {image && (
        <View style={styles.imageContainer}>
          <Image source={{ uri: image }} style={styles.image} />
          {prediction && (
            <View style={styles.predictionContainer}>
              <MaterialIcons
                name={prediction === "food" ? "check-circle" : "error"}
                size={24}
                color={
                  prediction === "food"
                    ? "green"
                    : prediction === "junk_food"
                    ? "orange"
                    : "red"
                }
              />
              <Text style={styles.predictionText}>
                This appears to be {prediction.replace("_", " ")}
                {confidence &&
                  `\nConfidence: ${(confidence * 100).toFixed(2)}%`}
              </Text>
            </View>
          )}
        </View>
      )}
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    flex: 1,
    alignItems: "center",
    padding: 20,
    backgroundColor: "#f5f5f5",
  },
  title: {
    fontSize: 24,
    fontWeight: "bold",
    marginBottom: 20,
  },
  imageContainer: {
    marginTop: 20,
    alignItems: "center",
  },
  image: {
    width: 300,
    height: 300,
    borderRadius: 10,
    marginBottom: 20,
  },
  loadingContainer: {
    marginTop: 20,
    alignItems: "center",
  },
  predictionContainer: {
    flexDirection: "row",
    alignItems: "center",
    backgroundColor: "white",
    padding: 10,
    borderRadius: 10,
    shadowColor: "#000",
    shadowOffset: {
      width: 0,
      height: 2,
    },
    shadowOpacity: 0.25,
    shadowRadius: 3.84,
    elevation: 5,
  },
  predictionText: {
    marginLeft: 10,
    fontSize: 16,
  },
});

export default Food;

================
File: app/app/(tabs)/index.tsx
================
import { Image, StyleSheet, Platform } from 'react-native';

import { HelloWave } from '@/components/HelloWave';
import ParallaxScrollView from '@/components/ParallaxScrollView';
import { ThemedText } from '@/components/ThemedText';
import { ThemedView } from '@/components/ThemedView';

export default function HomeScreen() {
  return (
    <ParallaxScrollView
      headerBackgroundColor={{ light: '#A1CEDC', dark: '#1D3D47' }}
      headerImage={
        <Image
          source={require('@/assets/images/partial-react-logo.png')}
          style={styles.reactLogo}
        />
      }>
      <ThemedView style={styles.titleContainer}>
        <ThemedText type="title">Welcome!</ThemedText>
        <HelloWave />
      </ThemedView>
      <ThemedView style={styles.stepContainer}>
        <ThemedText type="subtitle">Step 1: Try it</ThemedText>
        <ThemedText>
          Edit <ThemedText type="defaultSemiBold">app/(tabs)/index.tsx</ThemedText> to see changes.
          Press{' '}
          <ThemedText type="defaultSemiBold">
            {Platform.select({
              ios: 'cmd + d',
              android: 'cmd + m',
              web: 'F12'
            })}
          </ThemedText>{' '}
          to open developer tools.
        </ThemedText>
      </ThemedView>
      <ThemedView style={styles.stepContainer}>
        <ThemedText type="subtitle">Step 2: Explore</ThemedText>
        <ThemedText>
          Tap the Explore tab to learn more about what's included in this starter app.
        </ThemedText>
      </ThemedView>
      <ThemedView style={styles.stepContainer}>
        <ThemedText type="subtitle">Step 3: Get a fresh start</ThemedText>
        <ThemedText>
          When you're ready, run{' '}
          <ThemedText type="defaultSemiBold">npm run reset-project</ThemedText> to get a fresh{' '}
          <ThemedText type="defaultSemiBold">app</ThemedText> directory. This will move the current{' '}
          <ThemedText type="defaultSemiBold">app</ThemedText> to{' '}
          <ThemedText type="defaultSemiBold">app-example</ThemedText>.
        </ThemedText>
      </ThemedView>
    </ParallaxScrollView>
  );
}

const styles = StyleSheet.create({
  titleContainer: {
    flexDirection: 'row',
    alignItems: 'center',
    gap: 8,
  },
  stepContainer: {
    gap: 8,
    marginBottom: 8,
  },
  reactLogo: {
    height: 178,
    width: 290,
    bottom: 0,
    left: 0,
    position: 'absolute',
  },
});

================
File: app/app/_layout.tsx
================
import {DarkTheme, DefaultTheme, ThemeProvider} from "@react-navigation/native";
import {useFonts} from "expo-font";
import {Stack} from "expo-router";
import * as SplashScreen from "expo-splash-screen";
import {StatusBar} from "expo-status-bar";
import {useEffect} from "react";
import "react-native-reanimated";

import {useColorScheme} from "@/hooks/useColorScheme";
import Toast from "react-native-toast-message";
import {AuthProvider} from "@/contexts/authContext";

// Prevent the splash screen from auto-hiding before asset loading is complete.
SplashScreen.preventAutoHideAsync();

export default function RootLayout() {
  const colorScheme = useColorScheme();
  const [loaded] = useFonts({
    SpaceMono: require("../assets/fonts/SpaceMono-Regular.ttf"),
  });

  useEffect(() => {
    if (loaded) {
      SplashScreen.hideAsync();
    }
  }, [loaded]);

  if (!loaded) {
    return null;
  }

  return (
    <AuthProvider>
      <ThemeProvider value={colorScheme === "dark" ? DarkTheme : DefaultTheme}>
        <Stack>
          <Stack.Screen name="(tabs)" options={{headerShown: false}} />
          <Stack.Screen name="+not-found" />
        </Stack>
        <StatusBar style="auto" />
        <Toast />
      </ThemeProvider>
    </AuthProvider>
  );
}

================
File: app/app/+not-found.tsx
================
import {Link, Stack} from "expo-router";
import {StyleSheet} from "react-native";

import {ThemedText} from "@/components/ThemedText";
import {ThemedView} from "@/components/ThemedView";
import React from "react";

export default function NotFoundScreen() {
  return (
    <>
      <Stack.Screen options={{title: "Oops!"}} />
      <ThemedView style={styles.container}>
        <ThemedText type="title">This screen doesn't exist.</ThemedText>
        <Link href="/" style={styles.link}>
          <ThemedText type="link">Go to home screen!</ThemedText>
        </Link>
      </ThemedView>
    </>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    alignItems: "center",
    justifyContent: "center",
    padding: 20,
  },
  link: {
    marginTop: 15,
    paddingVertical: 15,
  },
});

================
File: app/app/protectedRoutes.tsx
================
import {AuthContext} from "@/contexts/authContext";
import {Stack} from "expo-router";
import React from "react";
import {useContext} from "react";

function ProtectedRoutes() {
  const {loading, user} = useContext(AuthContext);

  return (
    <Stack>
      {user ? (
        <>
          <Stack.Screen name="(tabs)" options={{headerShown: false}} />
          <Stack.Screen name="+not-found" />
        </>
      ) : (
        <Stack.Screen name="login" options={{headerShown: false}} />
      )}
    </Stack>
  );
}

================
File: app/app/sign-in.tsx
================
import {router} from "expo-router";
import {Text, View} from "react-native";


export default function SignIn() {
  const {signIn} = useSession();
  return (
    <View style={{flex: 1, justifyContent: "center", alignItems: "center"}}>
      <Text
        onPress={() => {
          signIn();
          // Navigate after signing in. You may want to tweak this to ensure sign-in is
          // successful before navigating.
          router.replace("/");
        }}
      >
        Sign In
      </Text>
    </View>
  );
}

================
File: app/components/__tests__/__snapshots__/ThemedText-test.tsx.snap
================
// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`renders correctly 1`] = `
<Text
  style={
    [
      {
        "color": "#11181C",
      },
      {
        "fontSize": 16,
        "lineHeight": 24,
      },
      undefined,
      undefined,
      undefined,
      undefined,
      undefined,
    ]
  }
>
  Snapshot test!
</Text>
`;

================
File: app/components/__tests__/ThemedText-test.tsx
================
import * as React from 'react';
import renderer from 'react-test-renderer';

import { ThemedText } from '../ThemedText';

it(`renders correctly`, () => {
  const tree = renderer.create(<ThemedText>Snapshot test!</ThemedText>).toJSON();

  expect(tree).toMatchSnapshot();
});

================
File: app/components/ui/IconSymbol.ios.tsx
================
import { SymbolView, SymbolViewProps, SymbolWeight } from 'expo-symbols';
import { StyleProp, ViewStyle } from 'react-native';

export function IconSymbol({
  name,
  size = 24,
  color,
  style,
  weight = 'regular',
}: {
  name: SymbolViewProps['name'];
  size?: number;
  color: string;
  style?: StyleProp<ViewStyle>;
  weight?: SymbolWeight;
}) {
  return (
    <SymbolView
      weight={weight}
      tintColor={color}
      resizeMode="scaleAspectFit"
      name={name}
      style={[
        {
          width: size,
          height: size,
        },
        style,
      ]}
    />
  );
}

================
File: app/components/ui/IconSymbol.tsx
================
// This file is a fallback for using MaterialIcons on Android and web.

import MaterialIcons from '@expo/vector-icons/MaterialIcons';
import { SymbolWeight } from 'expo-symbols';
import React from 'react';
import { OpaqueColorValue, StyleProp, ViewStyle } from 'react-native';

// Add your SFSymbol to MaterialIcons mappings here.
const MAPPING = {
  // See MaterialIcons here: https://icons.expo.fyi
  // See SF Symbols in the SF Symbols app on Mac.
  'house.fill': 'home',
  'paperplane.fill': 'send',
  'chevron.left.forwardslash.chevron.right': 'code',
  'chevron.right': 'chevron-right',
} as Partial<
  Record<
    import('expo-symbols').SymbolViewProps['name'],
    React.ComponentProps<typeof MaterialIcons>['name']
  >
>;

export type IconSymbolName = keyof typeof MAPPING;

/**
 * An icon component that uses native SFSymbols on iOS, and MaterialIcons on Android and web. This ensures a consistent look across platforms, and optimal resource usage.
 *
 * Icon `name`s are based on SFSymbols and require manual mapping to MaterialIcons.
 */
export function IconSymbol({
  name,
  size = 24,
  color,
  style,
}: {
  name: IconSymbolName;
  size?: number;
  color: string | OpaqueColorValue;
  style?: StyleProp<ViewStyle>;
  weight?: SymbolWeight;
}) {
  return <MaterialIcons color={color} size={size} name={MAPPING[name]} style={style} />;
}

================
File: app/components/ui/TabBarBackground.ios.tsx
================
import { useBottomTabBarHeight } from '@react-navigation/bottom-tabs';
import { BlurView } from 'expo-blur';
import { StyleSheet } from 'react-native';
import { useSafeAreaInsets } from 'react-native-safe-area-context';

export default function BlurTabBarBackground() {
  return (
    <BlurView
      // System chrome material automatically adapts to the system's theme
      // and matches the native tab bar appearance on iOS.
      tint="systemChromeMaterial"
      intensity={100}
      style={StyleSheet.absoluteFill}
    />
  );
}

export function useBottomTabOverflow() {
  const tabHeight = useBottomTabBarHeight();
  const { bottom } = useSafeAreaInsets();
  return tabHeight - bottom;
}

================
File: app/components/ui/TabBarBackground.tsx
================
// This is a shim for web and Android where the tab bar is generally opaque.
export default undefined;

export function useBottomTabOverflow() {
  return 0;
}

================
File: app/components/Collapsible.tsx
================
import { PropsWithChildren, useState } from 'react';
import { StyleSheet, TouchableOpacity } from 'react-native';

import { ThemedText } from '@/components/ThemedText';
import { ThemedView } from '@/components/ThemedView';
import { IconSymbol } from '@/components/ui/IconSymbol';
import { Colors } from '@/constants/Colors';
import { useColorScheme } from '@/hooks/useColorScheme';

export function Collapsible({ children, title }: PropsWithChildren & { title: string }) {
  const [isOpen, setIsOpen] = useState(false);
  const theme = useColorScheme() ?? 'light';

  return (
    <ThemedView>
      <TouchableOpacity
        style={styles.heading}
        onPress={() => setIsOpen((value) => !value)}
        activeOpacity={0.8}>
        <IconSymbol
          name="chevron.right"
          size={18}
          weight="medium"
          color={theme === 'light' ? Colors.light.icon : Colors.dark.icon}
          style={{ transform: [{ rotate: isOpen ? '90deg' : '0deg' }] }}
        />

        <ThemedText type="defaultSemiBold">{title}</ThemedText>
      </TouchableOpacity>
      {isOpen && <ThemedView style={styles.content}>{children}</ThemedView>}
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  heading: {
    flexDirection: 'row',
    alignItems: 'center',
    gap: 6,
  },
  content: {
    marginTop: 6,
    marginLeft: 24,
  },
});

================
File: app/components/ExternalLink.tsx
================
import { Link } from 'expo-router';
import { openBrowserAsync } from 'expo-web-browser';
import { type ComponentProps } from 'react';
import { Platform } from 'react-native';

type Props = Omit<ComponentProps<typeof Link>, 'href'> & { href: string };

export function ExternalLink({ href, ...rest }: Props) {
  return (
    <Link
      target="_blank"
      {...rest}
      href={href}
      onPress={async (event) => {
        if (Platform.OS !== 'web') {
          // Prevent the default behavior of linking to the default browser on native.
          event.preventDefault();
          // Open the link in an in-app browser.
          await openBrowserAsync(href);
        }
      }}
    />
  );
}

================
File: app/components/HapticTab.tsx
================
import { BottomTabBarButtonProps } from '@react-navigation/bottom-tabs';
import { PlatformPressable } from '@react-navigation/elements';
import * as Haptics from 'expo-haptics';

export function HapticTab(props: BottomTabBarButtonProps) {
  return (
    <PlatformPressable
      {...props}
      onPressIn={(ev) => {
        if (process.env.EXPO_OS === 'ios') {
          // Add a soft haptic feedback when pressing down on the tabs.
          Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light);
        }
        props.onPressIn?.(ev);
      }}
    />
  );
}

================
File: app/components/HelloWave.tsx
================
import { useEffect } from 'react';
import { StyleSheet } from 'react-native';
import Animated, {
  useSharedValue,
  useAnimatedStyle,
  withTiming,
  withRepeat,
  withSequence,
} from 'react-native-reanimated';

import { ThemedText } from '@/components/ThemedText';

export function HelloWave() {
  const rotationAnimation = useSharedValue(0);

  useEffect(() => {
    rotationAnimation.value = withRepeat(
      withSequence(withTiming(25, { duration: 150 }), withTiming(0, { duration: 150 })),
      4 // Run the animation 4 times
    );
  }, []);

  const animatedStyle = useAnimatedStyle(() => ({
    transform: [{ rotate: `${rotationAnimation.value}deg` }],
  }));

  return (
    <Animated.View style={animatedStyle}>
      <ThemedText style={styles.text}>ðŸ‘‹</ThemedText>
    </Animated.View>
  );
}

const styles = StyleSheet.create({
  text: {
    fontSize: 28,
    lineHeight: 32,
    marginTop: -6,
  },
});

================
File: app/components/ParallaxScrollView.tsx
================
import type { PropsWithChildren, ReactElement } from 'react';
import { StyleSheet } from 'react-native';
import Animated, {
  interpolate,
  useAnimatedRef,
  useAnimatedStyle,
  useScrollViewOffset,
} from 'react-native-reanimated';

import { ThemedView } from '@/components/ThemedView';
import { useBottomTabOverflow } from '@/components/ui/TabBarBackground';
import { useColorScheme } from '@/hooks/useColorScheme';

const HEADER_HEIGHT = 250;

type Props = PropsWithChildren<{
  headerImage: ReactElement;
  headerBackgroundColor: { dark: string; light: string };
}>;

export default function ParallaxScrollView({
  children,
  headerImage,
  headerBackgroundColor,
}: Props) {
  const colorScheme = useColorScheme() ?? 'light';
  const scrollRef = useAnimatedRef<Animated.ScrollView>();
  const scrollOffset = useScrollViewOffset(scrollRef);
  const bottom = useBottomTabOverflow();
  const headerAnimatedStyle = useAnimatedStyle(() => {
    return {
      transform: [
        {
          translateY: interpolate(
            scrollOffset.value,
            [-HEADER_HEIGHT, 0, HEADER_HEIGHT],
            [-HEADER_HEIGHT / 2, 0, HEADER_HEIGHT * 0.75]
          ),
        },
        {
          scale: interpolate(scrollOffset.value, [-HEADER_HEIGHT, 0, HEADER_HEIGHT], [2, 1, 1]),
        },
      ],
    };
  });

  return (
    <ThemedView style={styles.container}>
      <Animated.ScrollView
        ref={scrollRef}
        scrollEventThrottle={16}
        scrollIndicatorInsets={{ bottom }}
        contentContainerStyle={{ paddingBottom: bottom }}>
        <Animated.View
          style={[
            styles.header,
            { backgroundColor: headerBackgroundColor[colorScheme] },
            headerAnimatedStyle,
          ]}>
          {headerImage}
        </Animated.View>
        <ThemedView style={styles.content}>{children}</ThemedView>
      </Animated.ScrollView>
    </ThemedView>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
  },
  header: {
    height: HEADER_HEIGHT,
    overflow: 'hidden',
  },
  content: {
    flex: 1,
    padding: 32,
    gap: 16,
    overflow: 'hidden',
  },
});

================
File: app/components/ThemedText.tsx
================
import { Text, type TextProps, StyleSheet } from 'react-native';

import { useThemeColor } from '@/hooks/useThemeColor';

export type ThemedTextProps = TextProps & {
  lightColor?: string;
  darkColor?: string;
  type?: 'default' | 'title' | 'defaultSemiBold' | 'subtitle' | 'link';
};

export function ThemedText({
  style,
  lightColor,
  darkColor,
  type = 'default',
  ...rest
}: ThemedTextProps) {
  const color = useThemeColor({ light: lightColor, dark: darkColor }, 'text');

  return (
    <Text
      style={[
        { color },
        type === 'default' ? styles.default : undefined,
        type === 'title' ? styles.title : undefined,
        type === 'defaultSemiBold' ? styles.defaultSemiBold : undefined,
        type === 'subtitle' ? styles.subtitle : undefined,
        type === 'link' ? styles.link : undefined,
        style,
      ]}
      {...rest}
    />
  );
}

const styles = StyleSheet.create({
  default: {
    fontSize: 16,
    lineHeight: 24,
  },
  defaultSemiBold: {
    fontSize: 16,
    lineHeight: 24,
    fontWeight: '600',
  },
  title: {
    fontSize: 32,
    fontWeight: 'bold',
    lineHeight: 32,
  },
  subtitle: {
    fontSize: 20,
    fontWeight: 'bold',
  },
  link: {
    lineHeight: 30,
    fontSize: 16,
    color: '#0a7ea4',
  },
});

================
File: app/components/ThemedView.tsx
================
import { View, type ViewProps } from 'react-native';

import { useThemeColor } from '@/hooks/useThemeColor';

export type ThemedViewProps = ViewProps & {
  lightColor?: string;
  darkColor?: string;
};

export function ThemedView({ style, lightColor, darkColor, ...otherProps }: ThemedViewProps) {
  const backgroundColor = useThemeColor({ light: lightColor, dark: darkColor }, 'background');

  return <View style={[{ backgroundColor }, style]} {...otherProps} />;
}

================
File: app/constants/Colors.ts
================
/**
 * Below are the colors that are used in the app. The colors are defined in the light and dark mode.
 * There are many other ways to style your app. For example, [Nativewind](https://www.nativewind.dev/), [Tamagui](https://tamagui.dev/), [unistyles](https://reactnativeunistyles.vercel.app), etc.
 */

const tintColorLight = '#0a7ea4';
const tintColorDark = '#fff';

export const Colors = {
  light: {
    text: '#11181C',
    background: '#fff',
    tint: tintColorLight,
    icon: '#687076',
    tabIconDefault: '#687076',
    tabIconSelected: tintColorLight,
  },
  dark: {
    text: '#ECEDEE',
    background: '#151718',
    tint: tintColorDark,
    icon: '#9BA1A6',
    tabIconDefault: '#9BA1A6',
    tabIconSelected: tintColorDark,
  },
};

================
File: app/contexts/authContext.tsx
================
import {getMe, Userlogin, UserRegister} from "@/services/authService";
import toastService from "@/services/toastService";
import AsyncStorage from "@react-native-async-storage/async-storage";
import {createContext, ReactNode, useEffect, useState} from "react";

type IAuthContext = {
  user: User | null;
  register: (registerInput: RegisterInput) => void;
  login: (email: string, password: string) => void;
  logout: () => void;
  loading: boolean;
};

export const AuthContext = createContext<IAuthContext>({
  user: null,
  register: () => {},
  login: () => {},
  logout: () => {},
  loading: false,
});

export const AuthProvider = ({children}: {children: ReactNode}) => {
  const [user, setUser] = useState<User | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    const fetchUser = async () => {
      const token = await AsyncStorage.getItem("token");
      if (token) {
        const user = await getMe();
        setUser(user);
      }
      setLoading(false);
    };
    fetchUser();
  });

  const register = async (registerInput: RegisterInput) => {
    try {
      const data = await UserRegister(registerInput);
      const token = data.token;
      await AsyncStorage.setItem("token", token);
      setUser(data.user);
    } catch (e: any) {
      toastService.success("Error", e.message);
    }
  };

  const login = async (email: string, password: string) => {
    try {
      const data = await Userlogin(email, password);
      const token = data.token;
      await AsyncStorage.setItem("token", token);
      setUser(data.user);
    } catch (e: any) {
      toastService.success("Error", e.message);
    }
  };

  const logout = () => {
    AsyncStorage.removeItem("token");
    setUser(null);
  };

  return (
    <AuthContext.Provider value={{user, register, login, logout, loading}}>
      {children}
    </AuthContext.Provider>
  );
};

================
File: app/hooks/useColorScheme.ts
================
export { useColorScheme } from 'react-native';

================
File: app/hooks/useColorScheme.web.ts
================
import { useEffect, useState } from 'react';
import { useColorScheme as useRNColorScheme } from 'react-native';

/**
 * To support static rendering, this value needs to be re-calculated on the client side for web
 */
export function useColorScheme() {
  const [hasHydrated, setHasHydrated] = useState(false);

  useEffect(() => {
    setHasHydrated(true);
  }, []);

  const colorScheme = useRNColorScheme();

  if (hasHydrated) {
    return colorScheme;
  }

  return 'light';
}

================
File: app/hooks/useThemeColor.ts
================
/**
 * Learn more about light and dark modes:
 * https://docs.expo.dev/guides/color-schemes/
 */

import { Colors } from '@/constants/Colors';
import { useColorScheme } from '@/hooks/useColorScheme';

export function useThemeColor(
  props: { light?: string; dark?: string },
  colorName: keyof typeof Colors.light & keyof typeof Colors.dark
) {
  const theme = useColorScheme() ?? 'light';
  const colorFromProps = props[theme];

  if (colorFromProps) {
    return colorFromProps;
  } else {
    return Colors[theme][colorName];
  }
}

================
File: app/scripts/reset-project.js
================
#!/usr/bin/env node

/**
 * This script is used to reset the project to a blank state.
 * It deletes or moves the /app, /components, /hooks, /scripts, and /constants directories to /app-example based on user input and creates a new /app directory with an index.tsx and _layout.tsx file.
 * You can remove the `reset-project` script from package.json and safely delete this file after running it.
 */

const fs = require("fs");
const path = require("path");
const readline = require("readline");

const root = process.cwd();
const oldDirs = ["app", "components", "hooks", "constants", "scripts"];
const exampleDir = "app-example";
const newAppDir = "app";
const exampleDirPath = path.join(root, exampleDir);

const indexContent = `import { Text, View } from "react-native";

export default function Index() {
  return (
    <View
      style={{
        flex: 1,
        justifyContent: "center",
        alignItems: "center",
      }}
    >
      <Text>Edit app/index.tsx to edit this screen.</Text>
    </View>
  );
}
`;

const layoutContent = `import { Stack } from "expo-router";

export default function RootLayout() {
  return <Stack />;
}
`;

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
});

const moveDirectories = async (userInput) => {
  try {
    if (userInput === "y") {
      // Create the app-example directory
      await fs.promises.mkdir(exampleDirPath, { recursive: true });
      console.log(`ðŸ“ /${exampleDir} directory created.`);
    }

    // Move old directories to new app-example directory or delete them
    for (const dir of oldDirs) {
      const oldDirPath = path.join(root, dir);
      if (fs.existsSync(oldDirPath)) {
        if (userInput === "y") {
          const newDirPath = path.join(root, exampleDir, dir);
          await fs.promises.rename(oldDirPath, newDirPath);
          console.log(`âž¡ï¸ /${dir} moved to /${exampleDir}/${dir}.`);
        } else {
          await fs.promises.rm(oldDirPath, { recursive: true, force: true });
          console.log(`âŒ /${dir} deleted.`);
        }
      } else {
        console.log(`âž¡ï¸ /${dir} does not exist, skipping.`);
      }
    }

    // Create new /app directory
    const newAppDirPath = path.join(root, newAppDir);
    await fs.promises.mkdir(newAppDirPath, { recursive: true });
    console.log("\nðŸ“ New /app directory created.");

    // Create index.tsx
    const indexPath = path.join(newAppDirPath, "index.tsx");
    await fs.promises.writeFile(indexPath, indexContent);
    console.log("ðŸ“„ app/index.tsx created.");

    // Create _layout.tsx
    const layoutPath = path.join(newAppDirPath, "_layout.tsx");
    await fs.promises.writeFile(layoutPath, layoutContent);
    console.log("ðŸ“„ app/_layout.tsx created.");

    console.log("\nâœ… Project reset complete. Next steps:");
    console.log(
      `1. Run \`npx expo start\` to start a development server.\n2. Edit app/index.tsx to edit the main screen.${
        userInput === "y"
          ? `\n3. Delete the /${exampleDir} directory when you're done referencing it.`
          : ""
      }`
    );
  } catch (error) {
    console.error(`âŒ Error during script execution: ${error.message}`);
  }
};

rl.question(
  "Do you want to move existing files to /app-example instead of deleting them? (Y/n): ",
  (answer) => {
    const userInput = answer.trim().toLowerCase() || "y";
    if (userInput === "y" || userInput === "n") {
      moveDirectories(userInput).finally(() => rl.close());
    } else {
      console.log("âŒ Invalid input. Please enter 'Y' or 'N'.");
      rl.close();
    }
  }
);

================
File: app/services/authService.ts
================
import httpService from "./httpService";

export const Userlogin = async (email: string, password: string) => {
  const res = await httpService.post("auth", {email, password});
  return res.data;
};

export const UserRegister = async (registerInput: RegisterInput) => {
  const res = await httpService.post("auth/register", registerInput);
  return res.data;
};

export const getMe = async () => {
  const res = await httpService.get("auth/me");
  return res.data;
};

================
File: app/services/httpService.ts
================
import axios from "axios";
import AsyncStorage from "@react-native-async-storage/async-storage";

// Create an Axios instance
const api = axios.create({
  baseURL: process.env.EXPO_PUBLIC_API_URL, // Replace with your actual API base URL
  timeout: 10000, // 10 seconds timeout
});

// Add a request interceptor
api.interceptors.request.use(
  async (config) => {
    try {
      const token = await AsyncStorage.getItem("token");
      if (token) {
        config.headers.Authorization = `Bearer ${token}`;
      }
    } catch (error) {
      console.error("Error getting token:", error);
    }
    return config;
  },
  (error) => {
    return Promise.reject(error);
  }
);

// Add a response interceptor
api.interceptors.response.use(
  (response) => {
    return response;
  },
  async (error) => {
    if (error.response && error.response.status === 401) {
      console.log("Unauthorized! Redirecting to login...");
      await AsyncStorage.removeItem("token");
    }
    return Promise.reject(error);
  }
);

export default {
  get: axios.get,
  post: axios.post,
  put: axios.put,
  delete: axios.delete,
  patch: axios.patch,
};

================
File: app/services/toastService.ts
================
import Toast from "react-native-toast-message";

const showToast = (
  type: string,
  text1: string,
  text2: string,
  position = "top",
  visibilityTime = 3000
) => {
  Toast.show({
    type,
    text1,
    text2,
    position: position as any,
    visibilityTime,
  });
};

const toastService = {
  success: (text1: string, text2: string) => showToast("success", text1, text2),
  error: (text1: string, text2: string) => showToast("error", text1, text2),
  info: (text1: string, text2: string) => showToast("info", text1, text2),
};

export default toastService;

================
File: app/types/user.ts
================
type User = {
  _id: string;
  email: string;
  firstName: string;
  lastName: string;
  petName: string;
  healthPoints: number;
  mood: "happy" | "sad" | "neutral";
};

type RegisterInput = {
  email: string;
  firstName: string;
  lastName: string;
  petName: string;
};

================
File: app/utils/bundleResourceIO.ts
================
// app/utils/bundleResourceIO.ts
import * as FileSystem from "expo-file-system";
import { Platform } from "react-native";

export async function bundleResourceIO(modelJSON: any, weightsManifest: any) {
  const modelArtifacts = Object.assign({}, modelJSON);
  modelArtifacts.weightSpecs = modelJSON.weightsManifest[0].weights;

  const weightData = new ArrayBuffer(0);

  // Load weights
  if (Platform.OS === "ios" || Platform.OS === "android") {
    const weightPaths = weightsManifest.map(
      (manifest: any) => manifest.paths[0]
    );

    for (const path of weightPaths) {
      const uri = FileSystem.documentDirectory + path;
      const { exists } = await FileSystem.getInfoAsync(uri);

      if (!exists) {
        throw new Error(`Weight file ${path} does not exist`);
      }

      const fileContent = await FileSystem.readAsStringAsync(uri, {
        encoding: FileSystem.EncodingType.Base64,
      });

      const buffer = Buffer.from(fileContent, "base64");
      const tempBuffer = new ArrayBuffer(weightData.byteLength + buffer.length);
      new Uint8Array(tempBuffer).set(new Uint8Array(weightData), 0);
      new Uint8Array(tempBuffer).set(
        new Uint8Array(buffer),
        weightData.byteLength
      );
      weightData = tempBuffer;
    }
  }

  modelArtifacts.weightData = weightData;

  return modelArtifacts;
}

================
File: app/.gitignore
================
# Learn more https://docs.github.com/en/get-started/getting-started-with-git/ignoring-files

# dependencies
node_modules/

# Expo
.expo/
dist/
web-build/
expo-env.d.ts

# Native
*.orig.*
*.jks
*.p8
*.p12
*.key
*.mobileprovision

# Metro
.metro-health-check*

# debug
npm-debug.*
yarn-debug.*
yarn-error.*

# macOS
.DS_Store
*.pem

# local env files
.env*.local

# typescript
*.tsbuildinfo

app-example
.env

================
File: app/app.json
================
{
  "expo": {
    "name": "app",
    "slug": "app",
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/images/icon.png",
    "scheme": "myapp",
    "userInterfaceStyle": "automatic",
    "newArchEnabled": true,
    "ios": {
      "supportsTablet": true
    },
    "android": {
      "adaptiveIcon": {
        "foregroundImage": "./assets/images/adaptive-icon.png",
        "backgroundColor": "#ffffff"
      }
    },
    "web": {
      "bundler": "metro",
      "output": "static",
      "favicon": "./assets/images/favicon.png"
    },
    "plugins": [
      "expo-router",
      [
        "expo-splash-screen",
        {
          "image": "./assets/images/splash-icon.png",
          "imageWidth": 200,
          "resizeMode": "contain",
          "backgroundColor": "#ffffff"
        }
      ]
    ],
    "experiments": {
      "typedRoutes": true
    },
    "assetBundlePatterns": [
      "**/*",
      "assets/images/*",
      "assets/ml/*",
      "assets/fonts/*"
    ]
  }
}

================
File: app/metro.config.js
================
// app/metro.config.js
const { getDefaultConfig } = require("@expo/metro-config");

const config = getDefaultConfig(__dirname);

module.exports = {
  ...config,
  resolver: {
    ...config.resolver,
    assetExts: [...config.resolver.assetExts, "bin", "tflite"],
  },
};

================
File: app/package.json
================
{
  "name": "app",
  "main": "expo-router/entry",
  "version": "1.0.0",
  "scripts": {
    "start": "expo start --tunnel",
    "reset-project": "node ./scripts/reset-project.js",
    "android": "expo start --android",
    "ios": "expo start --ios",
    "web": "expo start --web",
    "test": "jest --watchAll",
    "lint": "expo lint"
  },
  "jest": {
    "preset": "jest-expo"
  },
  "dependencies": {
    "@expo/ngrok": "^4.1.0",
    "@expo/vector-icons": "^14.0.2",
    "@react-native-async-storage/async-storage": "1.21.0",
    "@react-navigation/bottom-tabs": "^7.2.0",
    "@react-navigation/native": "^7.0.14",
    "@tensorflow/tfjs": "4.22.0",
    "@tensorflow/tfjs-backend-cpu": "4.22.0",
    "@tensorflow/tfjs-backend-webgl": "4.22.0",
    "@tensorflow/tfjs-converter": "4.22.0",
    "@tensorflow/tfjs-core": "4.22.0",
    "@tensorflow/tfjs-react-native": "1.0.0",
    "@tensorflow/tfjs-tflite": "^0.0.1-alpha.10",
    "axios": "^1.7.9",
    "expo": "~52.0.35",
    "expo-blur": "~14.0.3",
    "expo-camera": "~16.0.16",
    "expo-constants": "~17.0.6",
    "expo-file-system": "~18.0.10",
    "expo-font": "~13.0.3",
    "expo-gl": "~15.0.4",
    "expo-gl-cpp": "^11.4.0",
    "expo-haptics": "~14.0.1",
    "expo-image-manipulator": "~13.0.6",
    "expo-image-picker": "~16.0.6",
    "expo-linking": "~7.0.5",
    "expo-router": "~4.0.17",
    "expo-splash-screen": "~0.29.22",
    "expo-status-bar": "~2.0.1",
    "expo-symbols": "~0.2.2",
    "expo-system-ui": "~4.0.8",
    "expo-web-browser": "~14.0.2",
    "react": "18.3.1",
    "react-dom": "18.3.1",
    "react-native": "0.76.7",
    "react-native-dotenv": "^3.4.11",
    "react-native-fs": "^2.20.0",
    "react-native-gesture-handler": "~2.20.2",
    "react-native-image-resizer": "^1.4.5",
    "react-native-reanimated": "~3.16.1",
    "react-native-safe-area-context": "4.12.0",
    "react-native-screens": "~4.4.0",
    "react-native-toast-message": "^2.2.1",
    "react-native-web": "~0.19.13",
    "react-native-webview": "13.12.5"
  },
  "devDependencies": {
    "@babel/core": "^7.25.2",
    "@types/jest": "^29.5.12",
    "@types/react": "~18.3.12",
    "@types/react-native-dotenv": "^0.2.2",
    "@types/react-test-renderer": "^18.3.0",
    "jest": "^29.2.1",
    "jest-expo": "~52.0.4",
    "react-test-renderer": "18.3.1",
    "typescript": "^5.3.3"
  },
  "private": true
}

================
File: app/README.md
================
# Welcome to your Expo app ðŸ‘‹

This is an [Expo](https://expo.dev) project created with [`create-expo-app`](https://www.npmjs.com/package/create-expo-app).

## Get started

1. Install dependencies

   ```bash
   npm install
   ```

2. Start the app

   ```bash
    npx expo start
   ```

In the output, you'll find options to open the app in a

- [development build](https://docs.expo.dev/develop/development-builds/introduction/)
- [Android emulator](https://docs.expo.dev/workflow/android-studio-emulator/)
- [iOS simulator](https://docs.expo.dev/workflow/ios-simulator/)
- [Expo Go](https://expo.dev/go), a limited sandbox for trying out app development with Expo

You can start developing by editing the files inside the **app** directory. This project uses [file-based routing](https://docs.expo.dev/router/introduction).

## Get a fresh project

When you're ready, run:

```bash
npm run reset-project
```

This command will move the starter code to the **app-example** directory and create a blank **app** directory where you can start developing.

## Learn more

To learn more about developing your project with Expo, look at the following resources:

- [Expo documentation](https://docs.expo.dev/): Learn fundamentals, or go into advanced topics with our [guides](https://docs.expo.dev/guides).
- [Learn Expo tutorial](https://docs.expo.dev/tutorial/introduction/): Follow a step-by-step tutorial where you'll create a project that runs on Android, iOS, and the web.

## Join the community

Join our community of developers creating universal apps.

- [Expo on GitHub](https://github.com/expo/expo): View our open source platform and contribute.
- [Discord community](https://chat.expo.dev): Chat with Expo users and ask questions.

================
File: app/tsconfig.json
================
{
  "extends": "expo/tsconfig.base",
  "compilerOptions": {
    "target": "ES2020",
    "module": "ESNext",
    "moduleResolution": "node",
    "esModuleInterop": true,
    "strict": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "outDir": "./dist",
    "rootDir": "./",
    "types": ["node"],
    "resolveJsonModule": true,
    "paths": {
      "@/*": [
        "./*"
      ]
    }
  },
  "include": [
    "**/*.ts",
    "**/*.tsx",
    ".expo/types/**/*.ts",
    "expo-env.d.ts"
  ],
  "exclude": ["node_modules"]
}

================
File: backend/middleware/auth.ts
================
import jwt from "jsonwebtoken";

export const auth = (req, res, next) => {
  // Try to get token from cookies first (for web clients)
  let token = req.cookies?.jwt_token;

  // If not found, check Authorization header (for React Native)
  if (!token && req.headers.authorization) {
    const authHeader = req.headers.authorization;
    if (authHeader.startsWith("Bearer ")) {
      token = authHeader.split(" ")[1]; // Extract token from Bearer scheme
    }
  }

  if (!token) {
    return res.status(401).json({error: "Access denied. No token provided."});
  }

  const jwtPrivateKey = process.env.JWT_PRIVATE_KEY;
  if (!jwtPrivateKey) {
    return res.status(500).json({error: "JWT private key is not defined."});
  }

  try {
    const decoded = jwt.verify(token, jwtPrivateKey);
    req.user = decoded;
    next();
  } catch (ex) {
    res.status(400).json({error: "Invalid token."});
  }
};

================
File: backend/ml/src/api.py
================
import sys
import json
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import traceback

# Suppress TensorFlow progress bars
tf.get_logger().setLevel("ERROR")
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# Define categories
CATEGORIES = ["non_food", "food", "junk_food"]


def load_model():
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(current_dir, "../model/model_latest.h5")

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found at {model_path}")

        model = tf.keras.models.load_model(model_path, compile=False)
        return model
    except Exception as e:
        error_msg = f"Failed to load model: {str(e)}"
        print(json.dumps({"error": error_msg}))
        sys.exit(1)


def preprocess_image(image_path):
    try:
        if not os.path.exists(image_path):
            raise FileNotFoundError(f"Image file not found at {image_path}")

        image = Image.open(image_path)
        image = image.convert("RGB")
        image = image.resize((224, 224))
        image = np.array(image)
        image = image / 255.0
        image = np.expand_dims(image, axis=0)
        return image
    except Exception as e:
        error_msg = f"Failed to process image: {str(e)}"
        print(json.dumps({"error": error_msg}))
        sys.exit(1)


def main():
    try:
        if len(sys.argv) != 2:
            raise ValueError("Image path not provided")

        image_path = sys.argv[1]
        model = load_model()
        processed_image = preprocess_image(image_path)

        # Disable progress bar for prediction
        predictions = model.predict(processed_image, verbose=0)[0]
        predicted_class = np.argmax(predictions)
        confidence = float(predictions[predicted_class])

        result = {
            "category": CATEGORIES[predicted_class],
            "confidence": confidence,
            "all_probabilities": {
                cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
            },
        }

        print(json.dumps(result))

    except Exception as e:
        error_msg = f"Main execution failed: {str(e)}"
        print(json.dumps({"error": error_msg}))
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: backend/ml/src/convert_to_tflite.py
================
# backend/ml/src/convert_to_tflite_simple.py
import tensorflow as tf
import os


def convert_model_simple():
    try:
        # Load your existing model
        print("Loading model...")
        model_path = "../model/model_latest.h5"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found at {model_path}")

        model = tf.keras.models.load_model(model_path)
        print("Model loaded successfully")

        # Create a concrete function from the model
        print("Creating concrete function...")
        input_shape = model.input_shape
        concrete_func = tf.function(model).get_concrete_function(
            tf.TensorSpec(input_shape, tf.float32)
        )

        # Convert using from_concrete_functions
        print("Converting model...")
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])

        # Basic settings
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]

        tflite_model = converter.convert()

        # Save the converted model
        output_path = "../model/model_latest.tflite"
        with open(output_path, "wb") as f:
            f.write(tflite_model)

        print(f"Model converted successfully and saved to {output_path}")
        print(
            f"Original model size: {os.path.getsize(model_path) / (1024*1024):.2f} MB"
        )
        print(f"TFLite model size: {os.path.getsize(output_path) / (1024*1024):.2f} MB")

    except Exception as e:
        print(f"Error during conversion: {str(e)}")
        raise


if __name__ == "__main__":
    print("Starting model conversion...")
    convert_model_simple()

================
File: backend/ml/src/dataset.py
================
# dataset.py
import tensorflow as tf
import os
import numpy as np
from typing import Tuple, Dict


class FoodDataset:
    def __init__(
        self,
        data_dir: str,
        img_size: Tuple[int, int] = (224, 224),
        batch_size: int = 32,
    ):
        """
        Initialize FoodDataset with enhanced preprocessing and validation

        Args:
            data_dir: Root directory of the dataset
            img_size: Target size for images (height, width)
            batch_size: Batch size for training
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.categories = ["non_food", "healthy_food", "unhealthy_food"]

    def _parse_image(
        self, filename: tf.Tensor, label: tf.Tensor
    ) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Enhanced image parsing with robust augmentation and preprocessing
        """
        # Convert filename to string
        filename = tf.cast(filename, tf.string)
        label = tf.cast(label, tf.int32)

        # Read and decode image
        image = tf.io.read_file(filename)
        image = tf.image.decode_jpeg(image, channels=3)

        # Enhanced data augmentation pipeline
        image = tf.cast(image, tf.float32)

        # Random augmentations for training diversity
        image = tf.image.random_brightness(image, 0.2)
        image = tf.image.random_contrast(image, 0.8, 1.2)
        image = tf.image.random_saturation(image, 0.8, 1.2)
        image = tf.image.random_hue(image, 0.1)
        image = tf.image.random_flip_left_right(image)

        # 50% chance of additional augmentations
        if tf.random.uniform([]) > 0.5:
            image = tf.image.transpose(image)
            image = tf.image.random_flip_up_down(image)

        # Center crop before resize for consistent aspect ratio
        shape = tf.shape(image)
        min_dim = tf.minimum(shape[0], shape[1])
        image = tf.image.resize_with_crop_or_pad(image, min_dim, min_dim)

        # Resize to target size
        image = tf.image.resize(image, self.img_size)

        # Normalize pixel values
        image = tf.clip_by_value(image, 0.0, 255.0)
        image = image / 255.0

        # Convert label to one-hot encoding
        label = tf.one_hot(label, 3)

        return image, label

    def create_dataset(self, split: str = "training") -> tf.data.Dataset:
        """
        Create dataset with enhanced error handling and logging
        """
        split_dir = os.path.join(self.data_dir, split)

        # Initialize containers
        image_files = []
        labels = []
        category_counts = {category: 0 for category in self.categories}

        # Process each category
        for label, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)

            if not os.path.exists(category_dir):
                print(f"Warning: Directory not found: {category_dir}")
                continue

            # Get all valid image files
            valid_files = [
                os.path.join(category_dir, f)
                for f in os.listdir(category_dir)
                if f.lower().endswith((".jpg", ".jpeg", ".png"))
            ]

            category_counts[category] = len(valid_files)
            image_files.extend(valid_files)
            labels.extend([label] * len(valid_files))

        # Print dataset statistics
        print(f"\nDataset statistics for {split}:")
        print("-" * 50)
        for category, count in category_counts.items():
            print(f"{category}: {count} images")
        print(f"Total: {sum(category_counts.values())} images")

        # Verify dataset is not empty
        if not image_files:
            raise ValueError(f"No images found in {split_dir}")

        # Create TensorFlow dataset
        dataset = tf.data.Dataset.from_tensor_slices(
            (tf.constant(image_files), tf.constant(labels, dtype=tf.int32))
        )

        # Configure dataset for performance
        dataset = dataset.map(self._parse_image, num_parallel_calls=tf.data.AUTOTUNE)

        if split == "training":
            # Shuffle training data with larger buffer
            dataset = dataset.shuffle(
                buffer_size=min(50000, len(image_files)), reshuffle_each_iteration=True
            )

        # Optimize performance
        dataset = dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE).cache()

        return dataset

    def get_class_weights(self, split: str = "training") -> Dict[int, float]:
        """
        Calculate balanced class weights with improved handling of edge cases
        """
        split_dir = os.path.join(self.data_dir, split)

        # Count images in each class
        counts = {}
        for i, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)
            if os.path.exists(category_dir):
                counts[i] = len(
                    [
                        f
                        for f in os.listdir(category_dir)
                        if f.lower().endswith((".jpg", ".jpeg", ".png"))
                    ]
                )
            else:
                counts[i] = 0
                print(f"Warning: Directory not found: {category_dir}")

        # Calculate total samples
        total_samples = sum(counts.values())
        if total_samples == 0:
            raise ValueError(f"No images found in {split_dir}")

        # Calculate balanced weights
        n_classes = len(self.categories)
        weights = {}
        for class_idx, count in counts.items():
            if count == 0:
                weights[class_idx] = 1.0
            else:
                weights[class_idx] = total_samples / (n_classes * count)

        # Print weight distribution
        print(f"\nClass weights for {split}:")
        for i, category in enumerate(self.categories):
            print(f"{category}: {weights[i]:.4f}")

        return weights

    def validate_dataset(self) -> None:
        """
        Validate dataset integrity and balance
        """
        for split in ["training", "validation", "evaluation"]:
            split_dir = os.path.join(self.data_dir, split)
            if not os.path.exists(split_dir):
                print(f"Warning: Split directory not found: {split_dir}")
                continue

            total_images = 0
            corrupted_images = 0

            for category in self.categories:
                category_dir = os.path.join(split_dir, category)
                if not os.path.exists(category_dir):
                    print(f"Warning: Category directory not found: {category_dir}")
                    continue

                # Check each image
                for img_name in os.listdir(category_dir):
                    if not img_name.lower().endswith((".jpg", ".jpeg", ".png")):
                        continue

                    total_images += 1
                    img_path = os.path.join(category_dir, img_name)

                    try:
                        with tf.io.gfile.GFile(img_path, "rb") as fid:
                            image_data = fid.read()
                            _ = tf.image.decode_jpeg(image_data, channels=3)
                    except tf.errors.InvalidArgumentError:
                        print(f"Corrupted image found: {img_path}")
                        corrupted_images += 1

            print(f"\nDataset validation results for {split}:")
            print(f"Total images: {total_images}")
            print(f"Corrupted images: {corrupted_images}")

================
File: backend/ml/src/model.py
================
import tensorflow as tf
from tensorflow.keras import layers, models


def create_model(input_shape=(224, 224, 3)):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze the base model
    base_model.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Changed to 3 outputs with softmax
        ]
    )

    # Compile the model
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",  # Use categorical_crossentropy for multi-class
        metrics=["accuracy"],
    )

    return model


def create_model_with_fine_tuning(input_shape=(224, 224, 3), fine_tune_layers=30):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze early layers
    base_model.trainable = True
    for layer in base_model.layers[:-fine_tune_layers]:
        layer.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Three categories
        ]
    )

    # Compile with a lower learning rate for fine-tuning
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    return model


def get_model_summary(model):
    """Get model architecture summary"""
    trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.trainable_weights], axis=0)
    )
    non_trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.non_trainable_weights], axis=0)
    )

    print("\nModel Summary:")
    print(f"Total parameters: {trainable_params + non_trainable_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {non_trainable_params:,}")

    return {
        "total_params": trainable_params + non_trainable_params,
        "trainable_params": trainable_params,
        "non_trainable_params": non_trainable_params,
    }

================
File: backend/ml/src/organize_dataset.py
================
# organize_dataset.py
import os
import shutil
from pathlib import Path

# Define healthy and unhealthy categories from Food-101 Data Set
HEALTHY_FOODS = [
    "beef_carpaccio",
    "beef_tartare",
    "beet_salad",
    "bibimbap",
    "caesar_salad",
    "caprese_salad",
    "ceviche",
    "chicken_curry",
    "edamame",
    "eggs_benedict",
    "falafel",
    "fried_rice",
    "gnocchi",
    "greek_salad",
    "grilled_salmon",
    "gyoza",
    "huevos_rancheros",
    "lasagna",
    "miso_soup",
    "mussels",
    "omelette",
    "pad_thai",
    "paella",
    "pancakes",
    "pho",
    "ramen",
    "risotto",
    "sashimi",
    "scallops",
    "seaweed_salad",
    "shrimp_and_grits",
    "spring_rolls",
    "steak",
    "sushi",
    "tuna_tartare",
    "waffles",
]

UNHEALTHY_FOODS = [
    "apple_pie",
    "baby_back_ribs",
    "baklava",
    "beignets",
    "bread_pudding",
    "breakfast_burrito",
    "cannoli",
    "carrot_cake",
    "cheesecake",
    "chicken_wings",
    "chocolate_cake",
    "chocolate_mousse",
    "churros",
    "croque_madame",
    "cup_cakes",
    "donuts",
    "french_fries",
    "fried_calamari",
    "hamburger",
    "hot_dog",
    "ice_cream",
    "macaroni_and_cheese",
    "macarons",
    "nachos",
    "onion_rings",
    "pizza",
    "poutine",
    "red_velvet_cake",
]


def create_hybrid_dataset():
    # Define paths and output to our hybrid_dataset (combination of the two)
    food101_path = "../data/food-101/images"
    food5k_path = "../data/Food-5k"
    output_path = "../data/hybrid_dataset"

    # Create directory structure
    for split in ["training", "validation", "evaluation"]:
        os.makedirs(os.path.join(output_path, split, "healthy_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "non_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "unhealthy_food"), exist_ok=True)

    # Copy non-food images from Food-5K
    for split in ["training", "validation", "evaluation"]:
        src_dir = os.path.join(food5k_path, split, "non_food")
        dst_dir = os.path.join(output_path, split, "non_food")

        print(f"Copying non-food images from {split} set...")
        for img in os.listdir(src_dir):
            if img.endswith(".jpg"):
                shutil.copy2(os.path.join(src_dir, img), os.path.join(dst_dir, img))

    # Copy and categorize food images from Food-101
    total_images_per_category = 1000  # Adjust this number as needed

    print("\nCopying healthy food images from Food-101...")
    for category in HEALTHY_FOODS:
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        # Calculate split sizes
        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        # Split images
        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in healthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "healthy_food", f"{category}_{img}"
                    ),
                )

    print("\nCopying unhealthy food images from Food-101...")
    for category in UNHEALTHY_FOODS:
        # Same process for unhealthy foods
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in unhealthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "unhealthy_food", f"{category}_{img}"
                    ),
                )

    # Print final dataset statistics
    print("\nFinal Dataset Statistics:")
    for split in ["training", "validation", "evaluation"]:
        healthy_count = len(
            os.listdir(os.path.join(output_path, split, "healthy_food"))
        )
        non_food_count = len(os.listdir(os.path.join(output_path, split, "non_food")))
        unhealthy_count = len(
            os.listdir(os.path.join(output_path, split, "unhealthy_food"))
        )
        print(f"\n{split.capitalize()} set:")
        print(f"Healthy food images: {healthy_count}")
        print(f"Non-food images: {non_food_count}")
        print(f"Unhealthy food images: {unhealthy_count}")
        print(f"Total: {healthy_count + non_food_count + unhealthy_count}")


if __name__ == "__main__":
    print("Creating hybrid dataset...")
    create_hybrid_dataset()
    print("\nDone!")

================
File: backend/ml/src/repomix-output.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-16T11:29:22.232Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
api_lite.py
api.py
convert_to_tflite.py
dataset.py
model.py
organize_dataset.py
test_api.py
test_model_webcam.py
test_model.py
train.py

================================================================
Files
================================================================

================
File: api_lite.py
================
# backend/ml/src/api_lite.py
import tensorflow as tf
import numpy as np
from PIL import Image
import json
import sys

CATEGORIES = ["non_food", "food", "junk_food"]


def load_and_process_image(image_path):
    img = Image.open(image_path)
    img = img.resize((224, 224))
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, 0)
    return img_array.astype(np.float32)


def main():
    if len(sys.argv) != 2:
        print(json.dumps({"error": "Image path not provided"}))
        sys.exit(1)

    try:
        # Load TFLite model using TensorFlow
        interpreter = tf.lite.Interpreter(model_path="../model/model_latest.tflite")
        interpreter.allocate_tensors()

        # Get input/output details
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()

        # Process image
        image_path = sys.argv[1]
        input_data = load_and_process_image(image_path)

        # Set input tensor
        interpreter.set_tensor(input_details[0]["index"], input_data)

        # Run inference
        interpreter.invoke()

        # Get output tensor
        predictions = interpreter.get_tensor(output_details[0]["index"])[0]
        predicted_class = np.argmax(predictions)
        confidence = float(predictions[predicted_class])

        # Prepare response
        result = {
            "category": CATEGORIES[predicted_class],
            "confidence": confidence,
            "all_probabilities": {
                cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
            },
        }

        print(json.dumps(result))

    except Exception as e:
        print(json.dumps({"error": f"Prediction failed: {str(e)}"}))
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: api.py
================
import sys
import json
import tensorflow as tf
import numpy as np
from PIL import Image

# Define categories
CATEGORIES = ["non_food", "food", "junk_food"]


def load_model():
    # Load the model from the relative path
    model_path = "../model/model_latest.h5"
    try:
        return tf.keras.models.load_model(model_path)
    except Exception as e:
        print(json.dumps({"error": f"Failed to load model: {str(e)}"}))
        sys.exit(1)


def preprocess_image(image_path):
    try:
        # Open and preprocess the image
        image = Image.open(image_path)
        image = image.resize((224, 224))
        image = np.array(image) / 255.0
        image = np.expand_dims(image, axis=0)
        return image
    except Exception as e:
        print(json.dumps({"error": f"Failed to process image: {str(e)}"}))
        sys.exit(1)


def main():
    if len(sys.argv) != 2:
        print(json.dumps({"error": "Image path not provided"}))
        sys.exit(1)

    image_path = sys.argv[1]

    # Load model
    model = load_model()

    # Process image
    processed_image = preprocess_image(image_path)

    try:
        # Make prediction
        predictions = model.predict(processed_image)[0]
        predicted_class = np.argmax(predictions)
        confidence = float(predictions[predicted_class])

        # Prepare response
        result = {
            "category": CATEGORIES[predicted_class],
            "confidence": confidence,
            "all_probabilities": {
                cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
            },
        }

        # Print result as JSON string
        print(json.dumps(result))

    except Exception as e:
        print(json.dumps({"error": f"Prediction failed: {str(e)}"}))
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: convert_to_tflite.py
================
# backend/ml/src/convert_to_tflite_simple.py
import tensorflow as tf
import os


def convert_model_simple():
    try:
        # Load your existing model
        print("Loading model...")
        model_path = "../model/model_latest.h5"
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found at {model_path}")

        model = tf.keras.models.load_model(model_path)
        print("Model loaded successfully")

        # Create a concrete function from the model
        print("Creating concrete function...")
        input_shape = model.input_shape
        concrete_func = tf.function(model).get_concrete_function(
            tf.TensorSpec(input_shape, tf.float32)
        )

        # Convert using from_concrete_functions
        print("Converting model...")
        converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])

        # Basic settings
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]

        tflite_model = converter.convert()

        # Save the converted model
        output_path = "../model/model_latest.tflite"
        with open(output_path, "wb") as f:
            f.write(tflite_model)

        print(f"Model converted successfully and saved to {output_path}")
        print(
            f"Original model size: {os.path.getsize(model_path) / (1024*1024):.2f} MB"
        )
        print(f"TFLite model size: {os.path.getsize(output_path) / (1024*1024):.2f} MB")

    except Exception as e:
        print(f"Error during conversion: {str(e)}")
        raise


if __name__ == "__main__":
    print("Starting model conversion...")
    convert_model_simple()

================
File: dataset.py
================
# dataset.py
import tensorflow as tf
import os
import numpy as np
from typing import Tuple, Dict


class FoodDataset:
    def __init__(
        self,
        data_dir: str,
        img_size: Tuple[int, int] = (224, 224),
        batch_size: int = 32,
    ):
        """
        Initialize FoodDataset with enhanced preprocessing and validation

        Args:
            data_dir: Root directory of the dataset
            img_size: Target size for images (height, width)
            batch_size: Batch size for training
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.categories = ["non_food", "healthy_food", "unhealthy_food"]

    def _parse_image(
        self, filename: tf.Tensor, label: tf.Tensor
    ) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Enhanced image parsing with robust augmentation and preprocessing
        """
        # Convert filename to string
        filename = tf.cast(filename, tf.string)
        label = tf.cast(label, tf.int32)

        # Read and decode image
        image = tf.io.read_file(filename)
        image = tf.image.decode_jpeg(image, channels=3)

        # Enhanced data augmentation pipeline
        image = tf.cast(image, tf.float32)

        # Random augmentations for training diversity
        image = tf.image.random_brightness(image, 0.2)
        image = tf.image.random_contrast(image, 0.8, 1.2)
        image = tf.image.random_saturation(image, 0.8, 1.2)
        image = tf.image.random_hue(image, 0.1)
        image = tf.image.random_flip_left_right(image)

        # 50% chance of additional augmentations
        if tf.random.uniform([]) > 0.5:
            image = tf.image.transpose(image)
            image = tf.image.random_flip_up_down(image)

        # Center crop before resize for consistent aspect ratio
        shape = tf.shape(image)
        min_dim = tf.minimum(shape[0], shape[1])
        image = tf.image.resize_with_crop_or_pad(image, min_dim, min_dim)

        # Resize to target size
        image = tf.image.resize(image, self.img_size)

        # Normalize pixel values
        image = tf.clip_by_value(image, 0.0, 255.0)
        image = image / 255.0

        # Convert label to one-hot encoding
        label = tf.one_hot(label, 3)

        return image, label

    def create_dataset(self, split: str = "training") -> tf.data.Dataset:
        """
        Create dataset with enhanced error handling and logging
        """
        split_dir = os.path.join(self.data_dir, split)

        # Initialize containers
        image_files = []
        labels = []
        category_counts = {category: 0 for category in self.categories}

        # Process each category
        for label, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)

            if not os.path.exists(category_dir):
                print(f"Warning: Directory not found: {category_dir}")
                continue

            # Get all valid image files
            valid_files = [
                os.path.join(category_dir, f)
                for f in os.listdir(category_dir)
                if f.lower().endswith((".jpg", ".jpeg", ".png"))
            ]

            category_counts[category] = len(valid_files)
            image_files.extend(valid_files)
            labels.extend([label] * len(valid_files))

        # Print dataset statistics
        print(f"\nDataset statistics for {split}:")
        print("-" * 50)
        for category, count in category_counts.items():
            print(f"{category}: {count} images")
        print(f"Total: {sum(category_counts.values())} images")

        # Verify dataset is not empty
        if not image_files:
            raise ValueError(f"No images found in {split_dir}")

        # Create TensorFlow dataset
        dataset = tf.data.Dataset.from_tensor_slices(
            (tf.constant(image_files), tf.constant(labels, dtype=tf.int32))
        )

        # Configure dataset for performance
        dataset = dataset.map(self._parse_image, num_parallel_calls=tf.data.AUTOTUNE)

        if split == "training":
            # Shuffle training data with larger buffer
            dataset = dataset.shuffle(
                buffer_size=min(50000, len(image_files)), reshuffle_each_iteration=True
            )

        # Optimize performance
        dataset = dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE).cache()

        return dataset

    def get_class_weights(self, split: str = "training") -> Dict[int, float]:
        """
        Calculate balanced class weights with improved handling of edge cases
        """
        split_dir = os.path.join(self.data_dir, split)

        # Count images in each class
        counts = {}
        for i, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)
            if os.path.exists(category_dir):
                counts[i] = len(
                    [
                        f
                        for f in os.listdir(category_dir)
                        if f.lower().endswith((".jpg", ".jpeg", ".png"))
                    ]
                )
            else:
                counts[i] = 0
                print(f"Warning: Directory not found: {category_dir}")

        # Calculate total samples
        total_samples = sum(counts.values())
        if total_samples == 0:
            raise ValueError(f"No images found in {split_dir}")

        # Calculate balanced weights
        n_classes = len(self.categories)
        weights = {}
        for class_idx, count in counts.items():
            if count == 0:
                weights[class_idx] = 1.0
            else:
                weights[class_idx] = total_samples / (n_classes * count)

        # Print weight distribution
        print(f"\nClass weights for {split}:")
        for i, category in enumerate(self.categories):
            print(f"{category}: {weights[i]:.4f}")

        return weights

    def validate_dataset(self) -> None:
        """
        Validate dataset integrity and balance
        """
        for split in ["training", "validation", "evaluation"]:
            split_dir = os.path.join(self.data_dir, split)
            if not os.path.exists(split_dir):
                print(f"Warning: Split directory not found: {split_dir}")
                continue

            total_images = 0
            corrupted_images = 0

            for category in self.categories:
                category_dir = os.path.join(split_dir, category)
                if not os.path.exists(category_dir):
                    print(f"Warning: Category directory not found: {category_dir}")
                    continue

                # Check each image
                for img_name in os.listdir(category_dir):
                    if not img_name.lower().endswith((".jpg", ".jpeg", ".png")):
                        continue

                    total_images += 1
                    img_path = os.path.join(category_dir, img_name)

                    try:
                        with tf.io.gfile.GFile(img_path, "rb") as fid:
                            image_data = fid.read()
                            _ = tf.image.decode_jpeg(image_data, channels=3)
                    except tf.errors.InvalidArgumentError:
                        print(f"Corrupted image found: {img_path}")
                        corrupted_images += 1

            print(f"\nDataset validation results for {split}:")
            print(f"Total images: {total_images}")
            print(f"Corrupted images: {corrupted_images}")

================
File: model.py
================
import tensorflow as tf
from tensorflow.keras import layers, models


def create_model(input_shape=(224, 224, 3)):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze the base model
    base_model.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Changed to 3 outputs with softmax
        ]
    )

    # Compile the model
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",  # Use categorical_crossentropy for multi-class
        metrics=["accuracy"],
    )

    return model


def create_model_with_fine_tuning(input_shape=(224, 224, 3), fine_tune_layers=30):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze early layers
    base_model.trainable = True
    for layer in base_model.layers[:-fine_tune_layers]:
        layer.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Three categories
        ]
    )

    # Compile with a lower learning rate for fine-tuning
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    return model


def get_model_summary(model):
    """Get model architecture summary"""
    trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.trainable_weights], axis=0)
    )
    non_trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.non_trainable_weights], axis=0)
    )

    print("\nModel Summary:")
    print(f"Total parameters: {trainable_params + non_trainable_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {non_trainable_params:,}")

    return {
        "total_params": trainable_params + non_trainable_params,
        "trainable_params": trainable_params,
        "non_trainable_params": non_trainable_params,
    }

================
File: organize_dataset.py
================
# organize_dataset.py
import os
import shutil
from pathlib import Path

# Define healthy and unhealthy categories from Food-101 Data Set
HEALTHY_FOODS = [
    "beef_carpaccio",
    "beef_tartare",
    "beet_salad",
    "bibimbap",
    "caesar_salad",
    "caprese_salad",
    "ceviche",
    "chicken_curry",
    "edamame",
    "eggs_benedict",
    "falafel",
    "fried_rice",
    "gnocchi",
    "greek_salad",
    "grilled_salmon",
    "gyoza",
    "huevos_rancheros",
    "lasagna",
    "miso_soup",
    "mussels",
    "omelette",
    "pad_thai",
    "paella",
    "pancakes",
    "pho",
    "ramen",
    "risotto",
    "sashimi",
    "scallops",
    "seaweed_salad",
    "shrimp_and_grits",
    "spring_rolls",
    "steak",
    "sushi",
    "tuna_tartare",
    "waffles",
]

UNHEALTHY_FOODS = [
    "apple_pie",
    "baby_back_ribs",
    "baklava",
    "beignets",
    "bread_pudding",
    "breakfast_burrito",
    "cannoli",
    "carrot_cake",
    "cheesecake",
    "chicken_wings",
    "chocolate_cake",
    "chocolate_mousse",
    "churros",
    "croque_madame",
    "cup_cakes",
    "donuts",
    "french_fries",
    "fried_calamari",
    "hamburger",
    "hot_dog",
    "ice_cream",
    "macaroni_and_cheese",
    "macarons",
    "nachos",
    "onion_rings",
    "pizza",
    "poutine",
    "red_velvet_cake",
]


def create_hybrid_dataset():
    # Define paths and output to our hybrid_dataset (combination of the two)
    food101_path = "../data/food-101/images"
    food5k_path = "../data/Food-5k"
    output_path = "../data/hybrid_dataset"

    # Create directory structure
    for split in ["training", "validation", "evaluation"]:
        os.makedirs(os.path.join(output_path, split, "healthy_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "non_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "unhealthy_food"), exist_ok=True)

    # Copy non-food images from Food-5K
    for split in ["training", "validation", "evaluation"]:
        src_dir = os.path.join(food5k_path, split, "non_food")
        dst_dir = os.path.join(output_path, split, "non_food")

        print(f"Copying non-food images from {split} set...")
        for img in os.listdir(src_dir):
            if img.endswith(".jpg"):
                shutil.copy2(os.path.join(src_dir, img), os.path.join(dst_dir, img))

    # Copy and categorize food images from Food-101
    total_images_per_category = 1000  # Adjust this number as needed

    print("\nCopying healthy food images from Food-101...")
    for category in HEALTHY_FOODS:
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        # Calculate split sizes
        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        # Split images
        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in healthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "healthy_food", f"{category}_{img}"
                    ),
                )

    print("\nCopying unhealthy food images from Food-101...")
    for category in UNHEALTHY_FOODS:
        # Same process for unhealthy foods
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in unhealthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "unhealthy_food", f"{category}_{img}"
                    ),
                )

    # Print final dataset statistics
    print("\nFinal Dataset Statistics:")
    for split in ["training", "validation", "evaluation"]:
        healthy_count = len(
            os.listdir(os.path.join(output_path, split, "healthy_food"))
        )
        non_food_count = len(os.listdir(os.path.join(output_path, split, "non_food")))
        unhealthy_count = len(
            os.listdir(os.path.join(output_path, split, "unhealthy_food"))
        )
        print(f"\n{split.capitalize()} set:")
        print(f"Healthy food images: {healthy_count}")
        print(f"Non-food images: {non_food_count}")
        print(f"Unhealthy food images: {unhealthy_count}")
        print(f"Total: {healthy_count + non_food_count + unhealthy_count}")


if __name__ == "__main__":
    print("Creating hybrid dataset...")
    create_hybrid_dataset()
    print("\nDone!")

================
File: test_api.py
================
# test_api.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os


def load_and_preprocess_image(image_path):
    """Load and preprocess a single image"""
    img = Image.open(image_path)
    img = img.resize((224, 224))
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_images():
    """Test model predictions on test images"""
    # Categories
    CATEGORIES = ["non_food", "food", "junk_food"]

    # Load model
    print("Loading model...")
    model_path = "../model/model_latest.h5"
    try:
        model = tf.keras.models.load_model(model_path)
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    # Test directory
    test_dir = "../test_images"
    test_images = sorted(
        [f for f in os.listdir(test_dir) if f.endswith((".jpg", ".jpeg"))]
    )

    print("\nTesting images...")
    print("=" * 50)

    results = []
    for image_name in test_images:
        image_path = os.path.join(test_dir, image_name)
        try:
            # Process image
            img_array = load_and_preprocess_image(image_path)

            # Get predictions
            predictions = model.predict(img_array, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Store results
            result = {
                "image": image_name,
                "predicted": CATEGORIES[predicted_class],
                "confidence": confidence,
                "probabilities": {
                    cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
                },
            }
            results.append(result)

            # Print results
            print(f"\nImage: {image_name}")
            print(f"Predicted: {result['predicted']}")
            print(f"Confidence: {confidence:.4f}")
            print("Probabilities:")
            for cat, prob in result["probabilities"].items():
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    # Print summary
    print("\nSummary:")
    for category in CATEGORIES:
        count = sum(1 for r in results if r["predicted"] == category)
        print(f"{category}: {count} images")


if __name__ == "__main__":
    test_images()

================
File: test_model_webcam.py
================
import tensorflow as tf
import cv2
import numpy as np
import os
import sys


def load_and_preprocess_frame(frame):
    resized = cv2.resize(frame, (224, 224))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    normalized = rgb / 255.0
    batched = np.expand_dims(normalized, 0)
    return batched


def main():
    # Load the trained model
    print("Loading model...")
    model_path = "../model_latest.h5"
    if not os.path.exists(model_path):
        print(f"Error: Model not found at {model_path}")
        sys.exit(1)

    model = tf.keras.models.load_model(model_path)

    # Category labels and colors
    CATEGORIES = ["NOT FOOD", "HEALTHY FOOD", "UNHEALTHY FOOD"]
    COLORS = [
        (0, 0, 255),  # Red for NOT FOOD
        (0, 255, 0),  # Green for HEALTHY FOOD
        (0, 165, 255),  # Orange for UNHEALTHY FOOD (BGR format)
    ]

    print("Starting webcam...")
    cap = cv2.VideoCapture(0)

    # Check if camera opened successfully
    if not cap.isOpened():
        print("\nError: Could not open camera")
        print(
            "Please check your camera permissions in System Settings > Privacy & Security > Camera"
        )
        print("Make sure Terminal/Python has permission to access the camera")
        sys.exit(1)

    print("Camera accessed successfully!")
    print("Press 'q' to quit")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Failed to grab frame")
                break

            processed_frame = load_and_preprocess_frame(frame)
            predictions = model.predict(processed_frame, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Get all probabilities
            probabilities = {cat: prob for cat, prob in zip(CATEGORIES, predictions)}

            # Prepare text display
            text = CATEGORIES[predicted_class]
            confidence_text = f"Confidence: {confidence:.2f}"

            # Additional probabilities text
            prob_texts = [f"{cat}: {prob:.2f}" for cat, prob in probabilities.items()]

            # Background rectangle for text (make it larger for all probabilities)
            cv2.rectangle(frame, (10, 10), (300, 110), (0, 0, 0), -1)

            # Add main prediction and confidence
            cv2.putText(
                frame,
                text,
                (20, 35),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                COLORS[predicted_class],
                2,
            )
            cv2.putText(
                frame,
                confidence_text,
                (20, 60),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2,
            )

            # Add all probabilities
            y_offset = 85
            cv2.putText(
                frame,
                f"All: {' | '.join(prob_texts)}",
                (20, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.4,
                (255, 255, 255),
                1,
            )

            cv2.imshow("Food Detector", frame)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                print("\nQuitting...")
                break

    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"\nAn error occurred: {str(e)}")
    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()

================
File: test_model.py
================
# test_model.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import random


def load_and_preprocess_image(image_path):
    # Load image
    img = Image.open(image_path)
    # Resize to match model's expected input
    img = img.resize((224, 224))
    # Convert to array and normalize
    img_array = np.array(img) / 255.0
    # Add batch dimension
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_model():
    # Load the trained model
    print("Loading model...")
    model = tf.keras.models.load_model("../model_final.h5")

    # Print model summary
    print("\nModel Summary:")
    model.summary()

    # Paths for evaluation set
    eval_dir = "../data/hybrid_dataset/evaluation"
    healthy_food_dir = os.path.join(eval_dir, "healthy_food")
    non_food_dir = os.path.join(eval_dir, "non_food")
    unhealthy_food_dir = os.path.join(eval_dir, "unhealthy_food")

    # Get 5 random images from each category
    healthy_food_images = random.sample(
        [f for f in os.listdir(healthy_food_dir) if f.endswith(".jpg")], 5
    )
    non_food_images = random.sample(
        [f for f in os.listdir(non_food_dir) if f.endswith(".jpg")], 5
    )
    unhealthy_food_images = random.sample(
        [f for f in os.listdir(unhealthy_food_dir) if f.endswith(".jpg")], 5
    )

    categories = ["Non-Food", "Healthy Food", "Unhealthy Food"]

    def test_category(images, directory, category_name):
        print(f"\nTesting {category_name} Images:")
        print("=" * 50)
        for img_name in images:
            img_path = os.path.join(directory, img_name)
            img_array = load_and_preprocess_image(img_path)
            predictions = model.predict(img_array, verbose=0)[0]

            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            print(f"\nImage: {img_name}")
            print(f"Prediction: {categories[predicted_class]}")
            print(f"Confidence: {confidence:.4f}")
            print("All probabilities:")
            for cat, prob in zip(categories, predictions):
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

    # Test all categories
    test_category(healthy_food_images, healthy_food_dir, "Healthy Food")
    test_category(non_food_images, non_food_dir, "Non-Food")
    test_category(unhealthy_food_images, unhealthy_food_dir, "Unhealthy Food")


if __name__ == "__main__":
    print("Testing model on all three categories...")
    test_model()

================
File: train.py
================
# train.py
import tensorflow as tf
from tensorflow.keras import layers, models
from dataset import FoodDataset
import os
import shutil


def ensure_directories():
    """Create necessary directories for model saving"""
    # Create both src/checkpoints and checkpoints directories
    directories = ["checkpoints", "src/checkpoints"]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"Created directory: {directory}")


def save_model_with_verification(model, base_path, filename):
    """Save model and verify it exists in both locations"""
    # Save to primary location
    primary_path = os.path.join(base_path, filename)
    model.save(primary_path)
    print(f"Model saved to: {primary_path}")

    # Copy to src/checkpoints for compatibility
    src_path = os.path.join("src/checkpoints", filename)
    shutil.copy2(primary_path, src_path)
    print(f"Model copied to: {src_path}")

    return primary_path, src_path


def train():
    # Configuration
    DATA_DIR = "../data/hybrid_dataset"
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    EPOCHS = 15

    # Ensure directories exist
    ensure_directories()

    print(f"Loading data from: {os.path.abspath(DATA_DIR)}")

    # Create datasets
    dataset = FoodDataset(DATA_DIR, IMG_SIZE, BATCH_SIZE)
    train_ds = dataset.create_dataset("training")
    val_ds = dataset.create_dataset("validation")

    # Get class weights
    class_weights = dataset.get_class_weights("training")
    print("\nClass weights:", class_weights)

    # Create model
    print("\nCreating model...")
    model = models.Sequential(
        [
            tf.keras.applications.MobileNetV2(
                input_shape=(*IMG_SIZE, 3), include_top=False, weights="imagenet"
            ),
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.4),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(3, activation="softmax"),
        ]
    )

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    # Callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath="checkpoints/best_model.h5",
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
            verbose=1,
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy", patience=5, restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.2, patience=3, min_lr=1e-6
        ),
    ]

    # Train model
    print("\nStarting training...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=callbacks,
        class_weight=class_weights,
        verbose=1,
    )

    # Save final model with verification
    print("\nSaving final model...")
    try:
        primary_path, src_path = save_model_with_verification(
            model, "checkpoints", "model.h5"
        )
        print(f"Model successfully saved and verified at:")
        print(f"1. {primary_path}")
        print(f"2. {src_path}")
    except Exception as e:
        print(f"Error saving model: {str(e)}")

    return history, model


if __name__ == "__main__":
    print("Starting training process...")
    try:
        history, model = train()

        # Verify saved model exists
        expected_paths = ["checkpoints/model.h5", "src/checkpoints/model.h5"]
        for path in expected_paths:
            if os.path.exists(path):
                print(f"Verified: Model exists at {path}")
            else:
                print(f"Warning: Model not found at {path}")

    except Exception as e:
        print(f"Error during training: {str(e)}")

================
File: backend/ml/src/test_api.py
================
# test_api.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os


def load_and_preprocess_image(image_path):
    """Load and preprocess a single image"""
    img = Image.open(image_path)
    img = img.resize((224, 224))
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_images():
    """Test model predictions on test images"""
    # Categories
    CATEGORIES = ["non_food", "food", "junk_food"]

    # Load model
    print("Loading model...")
    model_path = "../model/model_latest.h5"
    try:
        model = tf.keras.models.load_model(model_path)
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    # Test directory
    test_dir = "../test_images"
    test_images = sorted(
        [f for f in os.listdir(test_dir) if f.endswith((".jpg", ".jpeg"))]
    )

    print("\nTesting images...")
    print("=" * 50)

    results = []
    for image_name in test_images:
        image_path = os.path.join(test_dir, image_name)
        try:
            # Process image
            img_array = load_and_preprocess_image(image_path)

            # Get predictions
            predictions = model.predict(img_array, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Store results
            result = {
                "image": image_name,
                "predicted": CATEGORIES[predicted_class],
                "confidence": confidence,
                "probabilities": {
                    cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
                },
            }
            results.append(result)

            # Print results
            print(f"\nImage: {image_name}")
            print(f"Predicted: {result['predicted']}")
            print(f"Confidence: {confidence:.4f}")
            print("Probabilities:")
            for cat, prob in result["probabilities"].items():
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    # Print summary
    print("\nSummary:")
    for category in CATEGORIES:
        count = sum(1 for r in results if r["predicted"] == category)
        print(f"{category}: {count} images")


if __name__ == "__main__":
    test_images()

================
File: backend/ml/src/test_model_webcam.py
================
import tensorflow as tf
import cv2
import numpy as np
import os
import sys


def load_and_preprocess_frame(frame):
    resized = cv2.resize(frame, (224, 224))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    normalized = rgb / 255.0
    batched = np.expand_dims(normalized, 0)
    return batched


def main():
    # Load the trained model
    print("Loading model...")
    model_path = "../model_latest.h5"
    if not os.path.exists(model_path):
        print(f"Error: Model not found at {model_path}")
        sys.exit(1)

    model = tf.keras.models.load_model(model_path)

    # Category labels and colors
    CATEGORIES = ["NOT FOOD", "HEALTHY FOOD", "UNHEALTHY FOOD"]
    COLORS = [
        (0, 0, 255),  # Red for NOT FOOD
        (0, 255, 0),  # Green for HEALTHY FOOD
        (0, 165, 255),  # Orange for UNHEALTHY FOOD (BGR format)
    ]

    print("Starting webcam...")
    cap = cv2.VideoCapture(0)

    # Check if camera opened successfully
    if not cap.isOpened():
        print("\nError: Could not open camera")
        print(
            "Please check your camera permissions in System Settings > Privacy & Security > Camera"
        )
        print("Make sure Terminal/Python has permission to access the camera")
        sys.exit(1)

    print("Camera accessed successfully!")
    print("Press 'q' to quit")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Failed to grab frame")
                break

            processed_frame = load_and_preprocess_frame(frame)
            predictions = model.predict(processed_frame, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Get all probabilities
            probabilities = {cat: prob for cat, prob in zip(CATEGORIES, predictions)}

            # Prepare text display
            text = CATEGORIES[predicted_class]
            confidence_text = f"Confidence: {confidence:.2f}"

            # Additional probabilities text
            prob_texts = [f"{cat}: {prob:.2f}" for cat, prob in probabilities.items()]

            # Background rectangle for text (make it larger for all probabilities)
            cv2.rectangle(frame, (10, 10), (300, 110), (0, 0, 0), -1)

            # Add main prediction and confidence
            cv2.putText(
                frame,
                text,
                (20, 35),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                COLORS[predicted_class],
                2,
            )
            cv2.putText(
                frame,
                confidence_text,
                (20, 60),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2,
            )

            # Add all probabilities
            y_offset = 85
            cv2.putText(
                frame,
                f"All: {' | '.join(prob_texts)}",
                (20, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.4,
                (255, 255, 255),
                1,
            )

            cv2.imshow("Food Detector", frame)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                print("\nQuitting...")
                break

    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"\nAn error occurred: {str(e)}")
    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()

================
File: backend/ml/src/test_model.py
================
# test_model.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import random


def load_and_preprocess_image(image_path):
    # Load image
    img = Image.open(image_path)
    # Resize to match model's expected input
    img = img.resize((224, 224))
    # Convert to array and normalize
    img_array = np.array(img) / 255.0
    # Add batch dimension
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_model():
    # Load the trained model
    print("Loading model...")
    model = tf.keras.models.load_model("../model_final.h5")

    # Print model summary
    print("\nModel Summary:")
    model.summary()

    # Paths for evaluation set
    eval_dir = "../data/hybrid_dataset/evaluation"
    healthy_food_dir = os.path.join(eval_dir, "healthy_food")
    non_food_dir = os.path.join(eval_dir, "non_food")
    unhealthy_food_dir = os.path.join(eval_dir, "unhealthy_food")

    # Get 5 random images from each category
    healthy_food_images = random.sample(
        [f for f in os.listdir(healthy_food_dir) if f.endswith(".jpg")], 5
    )
    non_food_images = random.sample(
        [f for f in os.listdir(non_food_dir) if f.endswith(".jpg")], 5
    )
    unhealthy_food_images = random.sample(
        [f for f in os.listdir(unhealthy_food_dir) if f.endswith(".jpg")], 5
    )

    categories = ["Non-Food", "Healthy Food", "Unhealthy Food"]

    def test_category(images, directory, category_name):
        print(f"\nTesting {category_name} Images:")
        print("=" * 50)
        for img_name in images:
            img_path = os.path.join(directory, img_name)
            img_array = load_and_preprocess_image(img_path)
            predictions = model.predict(img_array, verbose=0)[0]

            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            print(f"\nImage: {img_name}")
            print(f"Prediction: {categories[predicted_class]}")
            print(f"Confidence: {confidence:.4f}")
            print("All probabilities:")
            for cat, prob in zip(categories, predictions):
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

    # Test all categories
    test_category(healthy_food_images, healthy_food_dir, "Healthy Food")
    test_category(non_food_images, non_food_dir, "Non-Food")
    test_category(unhealthy_food_images, unhealthy_food_dir, "Unhealthy Food")


if __name__ == "__main__":
    print("Testing model on all three categories...")
    test_model()

================
File: backend/ml/src/train.py
================
# train.py
import tensorflow as tf
from tensorflow.keras import layers, models
from dataset import FoodDataset
import os
import shutil


def ensure_directories():
    """Create necessary directories for model saving"""
    # Create both src/checkpoints and checkpoints directories
    directories = ["checkpoints", "src/checkpoints"]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"Created directory: {directory}")


def save_model_with_verification(model, base_path, filename):
    """Save model and verify it exists in both locations"""
    # Save to primary location
    primary_path = os.path.join(base_path, filename)
    model.save(primary_path)
    print(f"Model saved to: {primary_path}")

    # Copy to src/checkpoints for compatibility
    src_path = os.path.join("src/checkpoints", filename)
    shutil.copy2(primary_path, src_path)
    print(f"Model copied to: {src_path}")

    return primary_path, src_path


def train():
    # Configuration
    DATA_DIR = "../data/hybrid_dataset"
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    EPOCHS = 15

    # Ensure directories exist
    ensure_directories()

    print(f"Loading data from: {os.path.abspath(DATA_DIR)}")

    # Create datasets
    dataset = FoodDataset(DATA_DIR, IMG_SIZE, BATCH_SIZE)
    train_ds = dataset.create_dataset("training")
    val_ds = dataset.create_dataset("validation")

    # Get class weights
    class_weights = dataset.get_class_weights("training")
    print("\nClass weights:", class_weights)

    # Create model
    print("\nCreating model...")
    model = models.Sequential(
        [
            tf.keras.applications.MobileNetV2(
                input_shape=(*IMG_SIZE, 3), include_top=False, weights="imagenet"
            ),
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.4),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(3, activation="softmax"),
        ]
    )

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    # Callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath="checkpoints/best_model.h5",
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
            verbose=1,
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy", patience=5, restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.2, patience=3, min_lr=1e-6
        ),
    ]

    # Train model
    print("\nStarting training...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=callbacks,
        class_weight=class_weights,
        verbose=1,
    )

    # Save final model with verification
    print("\nSaving final model...")
    try:
        primary_path, src_path = save_model_with_verification(
            model, "checkpoints", "model.h5"
        )
        print(f"Model successfully saved and verified at:")
        print(f"1. {primary_path}")
        print(f"2. {src_path}")
    except Exception as e:
        print(f"Error saving model: {str(e)}")

    return history, model


if __name__ == "__main__":
    print("Starting training process...")
    try:
        history, model = train()

        # Verify saved model exists
        expected_paths = ["checkpoints/model.h5", "src/checkpoints/model.h5"]
        for path in expected_paths:
            if os.path.exists(path):
                print(f"Verified: Model exists at {path}")
            else:
                print(f"Warning: Model not found at {path}")

    except Exception as e:
        print(f"Error during training: {str(e)}")

================
File: backend/ml/requirements.txt
================
asgiref==3.5.2
certifi==2022.9.24
charset-normalizer==2.1.1
click==8.1.3
confluent-kafka==2.4.0
contourpy==1.0.7
cycler==0.11.0
Django==4.1.3
djangorestframework==3.14.0
Flask==1.1.2
Flask-Login==0.5.0
Flask-SQLAlchemy==2.4.4
fonttools==4.39.4
future==1.0.0
greenlet==2.0.1
idna==3.4
iso8601==2.1.0
itsdangerous==2.1.2
Jinja2==3.1.2
khan-api-wrapper==0.0.18
kiwisolver==1.4.4
MarkupSafe==2.1.1
matplotlib==3.7.1
mpmath==1.2.1
numpy==1.23.5
packaging==23.1
pandas==2.0.2
Pillow==9.3.0
pybraille==1.0.0
pygame==2.1.2
pyparsing==3.0.9
pyserial==3.5
python-dateutil==2.8.2
pytube==12.1.2
pytz==2022.6
PyYAML==6.0.2
rauth==0.7.3
requests==2.28.1
scipy==1.9.3
serial==0.0.97
six==1.16.0
SQLAlchemy==1.4.44
sqlparse==0.4.3
sympy==1.11.1
tzdata==2023.3
urllib3==1.26.12
Werkzeug==2.2.2
tensorflow==2.11.0

================
File: backend/models/user.ts
================
import Joi from "joi";
import mongoose from "mongoose";
import jwt from "jsonwebtoken";

interface IUserDocument extends mongoose.Document {
  firstName: string;
  lastName: string;
  email: string;
  password: string;
  petName: string;
  healthPoints: number;
  mood: "happy" | "sad" | "neutral";
  generateAuthToken(): string;
}

const userSchema = new mongoose.Schema<IUserDocument>({
  firstName: {
    type: String,
    required: true,
    maxlength: 50,
  },
  lastName: {
    type: String,
    required: true,
    maxlength: 50,
  },
  email: {
    type: String,
    required: true,
    minlength: 5,
    maxlength: 255,
    unique: true,
  },
  password: {
    type: String,
    required: true,
    minlength: 5,
    maxlength: 1024,
  },
  petName: {
    type: String,
    required: true,
    minlength: 1,
    maxlength: 255,
  },
  healthPoints: {
    type: Number,
    default: 100,
  },
  mood: {
    type: String,
    enum: ["happy", "sad", "neutral"],
    default: "neutral",
  },
});

userSchema.methods.generateAuthToken = function () {
  const token = jwt.sign(
    {
      _id: this._id,
      firstName: this.firstName,
      lastName: this.lastName,
      role: "user",
    },
    process.env.JWT_PRIVATE_KEY || "this is a secret key"
  );
  return token;
};

export const User = mongoose.model("users", userSchema);

export function validateUser(user) {
  const schema = Joi.object({
    firstName: Joi.string().max(50).required(),
    lastName: Joi.string().max(50).required(),
    email: Joi.string().min(5).max(255).required().email(),
    password: Joi.string().min(5).max(255).required(),
    petName: Joi.string().min(1).max(255).required(),
  });
  return schema.validate(user);
}

================
File: backend/repos/foodRepo.ts
================
import multer, { FileFilterCallback } from "multer";
import path from "path";
import fs from "fs";
import { Request } from "express";
import { fileURLToPath } from "url";

// Types
interface MulterFile {
  fieldname: string;
  originalname: string;
  encoding: string;
  mimetype: string;
  size: number;
  destination: string;
  filename: string;
  path: string;
  buffer: Buffer;
}

// Fix for __dirname in ES modules
const currentFilePath = fileURLToPath(import.meta.url);
const currentDirPath = path.dirname(currentFilePath);

// Create uploads directory path
const uploadPath = path.join(currentDirPath, "..", "ml", "uploads");

// Ensure uploads directory exists
if (!fs.existsSync(uploadPath)) {
  fs.mkdirSync(uploadPath, { recursive: true });
  console.log(`Created uploads directory at: ${uploadPath}`);
}

const storage = multer.diskStorage({
  destination: function (
    _req: Request,
    _file: MulterFile,
    cb: (error: Error | null, destination: string) => void
  ) {
    cb(null, uploadPath);
  },
  filename: function (
    _req: Request,
    file: MulterFile,
    cb: (error: Error | null, filename: string) => void
  ) {
    const uniqueSuffix = Date.now() + "-" + Math.round(Math.random() * 1e9);
    cb(null, uniqueSuffix + "-" + file.originalname);
  },
});

// Add file filter for images
const fileFilter = (
  _req: Request,
  file: MulterFile,
  cb: FileFilterCallback
) => {
  const allowedMimes = ["image/jpeg", "image/png", "image/jpg"];
  if (allowedMimes.includes(file.mimetype)) {
    cb(null, true);
  } else {
    cb(new Error("Invalid file type. Only JPEG and PNG are allowed."));
  }
};

// Create multer instance with configuration
const upload = multer({
  storage,
  fileFilter,
  limits: {
    fileSize: 50 * 1024 * 1024, // 5MB limit
  },
});

export const singleFoodUpload = upload.single("photo");

export async function processFoodFile(file: MulterFile) {
  const result = await fetch("http://localhost:4000/api/ml/classify", {
    method: "POST",
    body: JSON.stringify({
      argument: file.path
    }),
    headers: {
      "Content-Type": "application/json"
    }
  });

  const data = await result.json();

  return {
    data
  };
}

================
File: backend/repos/stateRepo.ts
================
import {User} from "../models/user";

export const getStates = async (req, res) => {
  const userId = process.env.DEMO_USER_ID;
  const user = await User.findById(userId);
  if (!user) return res.status(404).json({error: "User not found."});
  return res.status(200).json({HP: user.healthPoints, mood: user.mood});
};

================
File: backend/repos/userRepo.ts
================
import {User, validateUser} from "../models/user.js";
import Joi from "joi";
import bcrypt from "bcryptjs";
const {hash, genSalt, compare} = bcrypt;
import _ from "lodash";

export const login = async (req, res) => {
  const {error} = validate(req.body);
  if (error) return res.status(400).json({error: error.details[0].message});

  let user = await User.findOne({email: req.body.email});
  if (!user) return res.status(400).json({error: "Invalid email or password."});

  if (!user.password)
    return res.status(400).json({error: "Invalid email or password."});

  const validPassword = await compare(req.body.password, user.password);
  if (!validPassword)
    return res.status(400).json({error: "Invalid email or password."});

  const token = user.generateAuthToken();

  res.status(200).json({
    message: "Login successful",
    token,
    user: {
      _id: user._id,
      email: user.email,
      firstName: user.firstName,
      lastName: user.lastName,
      petName: user.petName,
      healthPoints: user.healthPoints,
      mood: user.mood,
    },
  });
};

export const register = async (req, res) => {
  const {error} = validateUser(req.body);
  if (error) return res.status(400).send(error.details[0].message);

  let user = await User.findOne({email: req.body.email});
  if (user) return res.status(400).send("User already registered.");

  user = new User(
    _.pick(req.body, ["firstName", "lastName", "email", "password", "petName"])
  );
  const salt = await genSalt(10);
  user.password = await hash(user.password, salt);
  await user.save();

  const token = user.generateAuthToken();

  return res.status(201).json({
    message: "Registration successful",
    token,
    user: _.pick(user, [
      "_id",
      "firstName",
      "lastName",
      "email",
      "petName",
      "healthPoints",
      "mood",
    ]),
  });
};

export const getMe = async (req, res) => {
  const user = await User.findById(req.user._id).select("-password");
  return res.status(200).json({
    user: _.pick(user, [
      "_id",
      "firstName",
      "lastName",
      "email",
      "petName",
      "healthPoints",
      "mood",
    ]),
  });
};

function validate(req) {
  const schema = {
    email: Joi.string().min(5).max(255).required().email(),
    password: Joi.string().min(5).max(255).required(),
  };

  return Joi.object(schema).validate(req.body);
}

export const test = (req, res) => {
  return res.status(200).json({message: "Test route"});
};

================
File: backend/routes/auth.ts
================
import {auth} from "./../middleware/auth";
import express from "express";
import {getMe, login, register, test} from "../repos/userRepo";

const router = express.Router();

router.post("/", login);
router.post("/register", register);
router.get("/test", auth, test);
router.get("/me", auth, getMe);

export default router;

================
File: backend/routes/food.ts
================
import { Router, Request, Response } from "express";
import { singleFoodUpload, processFoodFile } from "../repos/foodRepo.js";

const router = Router();

// POST /food
router.post("/", singleFoodUpload, (req: Request, res: Response) => {
  try {
    console.log("Received request with file:", req.file);

    if (!req.file) {
      console.log("No file in request");
      return res.status(400).json({ error: "No file uploaded." });
    }

    // Process the uploaded file
    console.log("Processing file...");
    console.log("File size:", req.file.size);
    console.log("File path:", req.file);
    const processedFile = processFoodFile(req.file);
    console.log("Processed file result:", processedFile);

    return res.json(processedFile);
  } catch (error) {
    console.error("Error in /food route:", error);
    return res.status(500).json({
      error: "Internal server error.",
      details: error instanceof Error ? error.message : "Unknown error",
    });
  }
});

export default router;

================
File: backend/routes/ml.ts
================
// backend/routes/ml.ts
import express, { Request, Response } from "express";
import { spawn } from "child_process";
import path from "path";
import multer from "multer";
import { fileURLToPath } from "url";
import fs from "fs/promises";
import fsSync from "fs"; // Add this import for synchronous file operations

// Types
interface ClassificationResult {
  category: string;
  confidence: number;
  all_probabilities: {
    [key: string]: number;
  };
}

interface ErrorResponse {
  error: string;
  details?: unknown;
}

interface ModelMetadata {
  version: string;
  size: number;
  lastModified: string;
  inputShape: number[];
  labels: string[];
}

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Ensure uploads directory exists
const uploadsDir = path.join(__dirname, "../uploads");
fs.mkdir(uploadsDir, { recursive: true }).catch(console.error);

// Multer configuration
const storage = multer.diskStorage({
  destination: uploadsDir,
  filename: (req, file, cb) => {
    const uniqueSuffix = `${Date.now()}-${Math.round(Math.random() * 1e9)}`;
    cb(null, `${uniqueSuffix}-${file.originalname}`);
  },
});

const fileFilter = (
  req: Express.Request,
  file: Express.Multer.File,
  cb: multer.FileFilterCallback
) => {
  const allowedTypes = ["image/jpeg", "image/png", "image/jpg"];
  if (allowedTypes.includes(file.mimetype)) {
    cb(null, true);
  } else {
    cb(new Error("Invalid file type. Only JPEG and PNG are allowed."));
  }
};

const upload = multer({
  storage,
  fileFilter,
  limits: { fileSize: 5 * 1024 * 1024 },
});

const router = express.Router();

// Serve TFLite model
router.get("/model", async (req: Request, res: Response) => {
  const modelPath = path.join(__dirname, "../ml/model/model_latest.tflite");

  try {
    // Check if file exists
    const exists = await fs
      .access(modelPath)
      .then(() => true)
      .catch(() => false);

    if (!exists) {
      console.error("Model file not found at:", modelPath);
      return res.status(404).json({ error: "Model file not found" });
    }

    // Get file stats
    const stats = await fs.stat(modelPath);
    console.log("Serving model file:", {
      path: modelPath,
      size: stats.size,
    });

    // Set appropriate headers
    res.setHeader("Content-Type", "application/octet-stream");
    res.setHeader("Content-Length", stats.size);
    res.setHeader(
      "Content-Disposition",
      "attachment; filename=model_latest.tflite"
    );

    // Create read stream using synchronous fs
    const fileStream = fsSync.createReadStream(modelPath);

    // Handle stream errors
    fileStream.on("error", (error) => {
      console.error("Stream error:", error);
      if (!res.headersSent) {
        res.status(500).json({
          error: "Failed to stream model",
          details: error.message,
        });
      }
    });

    // Pipe the file to response
    fileStream.pipe(res);
  } catch (error) {
    console.error("Error serving model:", error);
    if (!res.headersSent) {
      res.status(500).json({
        error: "Failed to serve model",
        details: error instanceof Error ? error.message : String(error),
      });
    }
  }
});

// Get model metadata
router.get("/model/metadata", async (req: Request, res: Response) => {
  const modelPath = path.join(__dirname, "../ml/model/model_latest.tflite");

  try {
    const stats = await fs.stat(modelPath);
    const metadata: ModelMetadata = {
      version: "1.0.0",
      size: stats.size,
      lastModified: stats.mtime.toISOString(),
      inputShape: [1, 224, 224, 3],
      labels: ["non_food", "food", "junk_food"],
    };

    res.json(metadata);
  } catch (error) {
    res.status(500).json({
      error: "Failed to get model metadata",
      details: error instanceof Error ? error.message : String(error),
    });
  }
});

// Your existing predict endpoint for server-side processing
router.post(
  "/predict",
  upload.single("image"),
  async (
    req: Request,
    res: Response<ClassificationResult | ErrorResponse>
  ): Promise<void> => {
    console.log("Received prediction request");

    if (!req.file) {
      console.log("No file received in request");
      res.status(400).json({ error: "No image provided" });
      return;
    }

    console.log("Received file:", {
      filename: req.file.filename,
      path: req.file.path,
      mimetype: req.file.mimetype,
      size: req.file.size,
    });

    try {
      const scriptPath = path.join(__dirname, "../ml/src/api.py");
      console.log("Python script path:", scriptPath);

      const result = await runPythonScript(scriptPath, [req.file.path]);
      console.log("Python script result:", result);

      const classification: ClassificationResult = JSON.parse(result);
      res.json(classification);
    } catch (error) {
      console.error("Classification error:", error);
      res.status(500).json({
        error: "Classification failed",
        details: error instanceof Error ? error.message : String(error),
      });
    } finally {
      if (req.file) {
        await cleanupFile(req.file.path);
      }
    }
  }
);

const runPythonScript = async (
  scriptPath: string,
  args: string[] = []
): Promise<string> => {
  return new Promise((resolve, reject) => {
    console.log("Running Python script:", {
      scriptPath,
      args,
      cwd: process.cwd(),
    });

    const pythonCommand = process.platform === "darwin" ? "python3" : "python";
    const pythonProcess = spawn(pythonCommand, [scriptPath, ...args], {
      cwd: process.cwd(),
    });

    let result = "";
    let errorOutput = "";

    pythonProcess.stdout.on("data", (data) => {
      const output = data.toString();
      console.log("Python stdout:", output);
      // Only append if it looks like JSON
      if (output.trim().startsWith("{")) {
        result = output.trim();
      }
    });

    pythonProcess.stderr.on("data", (data) => {
      const error = data.toString();
      console.error("Python stderr:", error);
      errorOutput += error;
    });

    pythonProcess.on("close", (code) => {
      console.log("Python process closed with code:", code);

      if (code === 0 && result) {
        try {
          // Verify it's valid JSON
          JSON.parse(result);
          resolve(result);
        } catch (e) {
          reject(new Error(`Invalid JSON output: ${result}`));
        }
      } else {
        reject(
          new Error(
            `Python script failed with code ${code}: ${errorOutput || result}`
          )
        );
      }
    });

    pythonProcess.on("error", (error) => {
      console.error("Process spawn error:", error);
      reject(error);
    });
  });
};

// Cleanup function
const cleanupFile = async (filePath: string): Promise<void> => {
  try {
    await fs.unlink(filePath);
  } catch (error) {
    console.error(`Failed to cleanup file ${filePath}:`, error);
  }
};

export default router;

================
File: backend/routes/pet.ts
================
// backend/routes/pet.ts
import express, { Request, Response } from "express";
import { analyzePetInteraction } from "../services/llmService";
import { User } from "../models/user";

const router = express.Router();

const DEMO_USER_ID = process.env.DEMO_USER_ID;

const getDemoUser = async () => {
  let user = await User.findById(DEMO_USER_ID);
  if (!user) {
    // Create demo user if none exists
    user = await User.create({
      _id: DEMO_USER_ID, // Use the specific ID
      firstName: "Demo",
      lastName: "User",
      email: "demo@example.com",
      password: "demo123", // In a real app, this should be hashed
      petName: "Kitty",
      healthPoints: 100,
      mood: "neutral",
    });
  }
  return user;
};

router.post("/interact", (req: Request, res: Response) => {
  (async () => {
    try {
      const { speech } = req.body;
      if (!speech) {
        return res.status(400).json({ error: "Speech content is required" });
      }

      // Analyze speech with LLM
      const analysis = await analyzePetInteraction(speech);

      // Get demo user
      const user = await getDemoUser();

      // Calculate new health points
      const newHP = Math.max(
        0,
        Math.min(100, user.healthPoints + analysis.moodImpact)
      );

      // Determine new mood based on health points
      let newMood: "happy" | "sad" | "neutral";
      if (newHP >= 70) {
        newMood = "happy";
      } else if (newHP <= 30) {
        newMood = "sad";
      } else {
        newMood = "neutral";
      }

      // Update user's pet state
      await User.findByIdAndUpdate(user._id, {
        healthPoints: newHP,
        mood: newMood,
      });

      // Return response
      return res.json({
        petState: {
          healthPoints: newHP,
          mood: newMood,
          petName: user.petName,
        },
        analysis,
      });
    } catch (error) {
      return res.status(500).json({
        error: "Failed to process pet interaction",
        details: error instanceof Error ? error.message : String(error),
      });
    }
  })();
});

router.get("/state", (req: Request, res: Response) => {
  (async () => {
    try {
      const user = await getDemoUser();

      return res.json({
        petName: user.petName,
        healthPoints: user.healthPoints,
        mood: user.mood,
      });
    } catch (error) {
      return res.status(500).json({
        error: "Failed to get pet state",
        details: error instanceof Error ? error.message : String(error),
      });
    }
  })();
});

export default router;

================
File: backend/routes/state.ts
================
import {auth} from "./../middleware/auth";
import express from "express";
import {login, register, test} from "../repos/userRepo";
import { getStates } from "../repos/stateRepo";

const router = express.Router();

router.get("/", getStates);

export default router;

================
File: backend/services/llmService.ts
================
// backend/services/llmService.ts
import OpenAI from "openai";
import dotenv from "dotenv";
dotenv.config();

if (!process.env.DEEPSEEK_API_KEY) {
  throw new Error("DEEPSEEK_API_KEY is not set in environment variables");
}

const openai = new OpenAI({
  apiKey: process.env.DEEPSEEK_API_KEY,
  baseURL: "https://api.deepseek.com",
});

interface PetAnalysis {
  moodImpact: number;
  sentiment: "positive" | "negative" | "neutral";
  catReaction: string;
}

export async function analyzePetInteraction(
  speech: string
): Promise<PetAnalysis> {
  try {
    console.log("ðŸ“ Processing speech:", speech);

    const response = await openai.chat.completions.create({
      model: "deepseek-chat",
      messages: [
        {
          role: "system",
          content: `You are an emotion analyzer for a virtual pet cat. Analyze the speech and return ONLY a JSON object.
          
For example:
For positive speech: {"moodImpact": 7, "sentiment": "positive", "catReaction": "Purrs loudly and rubs against your leg"}
For negative speech: {"moodImpact": -5, "sentiment": "negative", "catReaction": "Flattens ears and backs away"}
For neutral speech: {"moodImpact": 0, "sentiment": "neutral", "catReaction": "Glances briefly and continues current activity"}

Rules:
1. Positive speech (praise, love) = positive impact (1 to 10)
2. Negative speech (scolding, threats) = negative impact (-1 to -10)
3. Stronger emotions = stronger impact
4. Cat reactions must be realistic

Return only the JSON object, no additional text or formatting.`,
        },
        { role: "user", content: speech },
      ],
      temperature: 0.7,
      max_tokens: 150,
    });

    console.log("ðŸ¤– Raw LLM Response:", response.choices[0].message.content);

    const content = response.choices[0].message.content;
    if (!content) {
      throw new Error("Empty response from LLM");
    }

    // Clean up the response if needed
    const cleanContent = content
      .replace(/```json\n?/, "")
      .replace(/```\n?/, "")
      .trim();

    try {
      const analysis = JSON.parse(cleanContent);

      // Validate the response
      if (
        typeof analysis.moodImpact !== "number" ||
        !["positive", "negative", "neutral"].includes(analysis.sentiment) ||
        typeof analysis.catReaction !== "string"
      ) {
        throw new Error("Invalid response format from LLM");
      }

      return analysis;
    } catch (parseError) {
      console.error("Parse error:", parseError);
      console.error("Content that failed to parse:", cleanContent);
      throw new Error("Failed to parse LLM response");
    }
  } catch (error) {
    console.error("ðŸš¨ LLM Analysis failed:", error);
    // Return a fallback response instead of throwing
    return {
      moodImpact: 0,
      sentiment: "neutral",
      catReaction: "The cat seems unsure how to react.",
    };
  }
}

================
File: backend/startup/db.ts
================
import mongoose from "mongoose";

const uri = process.env.MONGO_URI;

if (!uri) {
  console.error("MONGODB_URI is not defined.");
  process.exit(1);
}

export async function connectToMongo() {
  try {
    await mongoose.connect(uri || "");
    console.log("Connected to MongoDB using Mongoose");
  } catch (err) {
    console.error("Error connecting to MongoDB with Mongoose:", err);
  }
}

================
File: backend/startup/routes.ts
================
import authRoutes from "../routes/auth.js";
import stateRoutes from "../routes/state.js";
import petRoutes from "../routes/pet.js";
import foodRoutes from "../routes/food.js";
import mlRoutes from "../routes/ml.js";
const setupRoutes = (app) => {
  app.use("/api/auth", authRoutes);
  app.use("/api/state", stateRoutes);
  app.use("/api/pet", petRoutes);
  app.use("/api/food", foodRoutes);
  app.use("/api/ml", mlRoutes);
};

export default setupRoutes;

================
File: backend/tests/llm-pet-test.js
================
// backend/tests/llm-pet-test.js
import fetch from "node-fetch";

const positiveSpeeches = [
  {
    description: "Loving and affectionate",
    speech:
      "You're the most precious kitty in the world! I love you so much and I'll always take care of you. You make me so happy every single day!",
  },
  {
    description: "Praising good behavior",
    speech:
      "What a good kitty you are! You're so well-behaved and gentle. I'm so proud of how you always use your scratching post!",
  },
  {
    description: "Gentle and soothing",
    speech:
      "Hey sweet baby, you're such a calm and beautiful cat. Your purring makes me feel so peaceful and loved.",
  },
];

const negativeSpeeches = [
  {
    description: "Harsh scolding",
    speech:
      "You're such a terrible cat! I can't believe you knocked everything off the table again! I regret getting you!",
  },
  {
    description: "Threatening",
    speech:
      "If you don't stop scratching the furniture, I'm going to get rid of you! You're the worst pet ever!",
  },
  {
    description: "Cold and dismissive",
    speech:
      "Just go away, I don't want you around. You're nothing but trouble and I wish I never got a cat.",
  },
];

async function testPetInteractions() {
  const baseUrl = "http://localhost:4000/api/pet";

  try {
    // Initial state
    console.log("\nðŸ“Š Getting initial state...");
    const stateRes = await fetch(`${baseUrl}/state`);
    const initialState = await stateRes.json();
    console.log("Initial state:", initialState);

    // Test positive interactions
    console.log("\nðŸ’š Testing Positive Interactions ðŸ’š");
    for (const test of positiveSpeeches) {
      console.log(`\nðŸ—£ Testing: ${test.description}`);
      const res = await fetch(`${baseUrl}/interact`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ speech: test.speech }),
      });
      const result = await res.json();
      console.log("Response:", result);
      console.log("Mood Impact:", result.analysis.moodImpact);
      console.log("Cat's Reaction:", result.analysis.catReaction);

      // Wait between tests
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }

    // Get intermediate state
    console.log("\nðŸ“Š Checking state after positive interactions...");
    const midStateRes = await fetch(`${baseUrl}/state`);
    const midState = await midStateRes.json();
    console.log("Current state:", midState);

    // Test negative interactions
    console.log("\nâ¤ï¸â€ðŸ©¹ Testing Negative Interactions â¤ï¸â€ðŸ©¹");
    for (const test of negativeSpeeches) {
      console.log(`\nðŸ—£ Testing: ${test.description}`);
      const res = await fetch(`${baseUrl}/interact`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ speech: test.speech }),
      });
      const result = await res.json();
      console.log("Response:", result);
      console.log("Mood Impact:", result.analysis.moodImpact);
      console.log("Cat's Reaction:", result.analysis.catReaction);

      // Wait between tests
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }

    // Get final state
    console.log("\nðŸ“Š Getting final state...");
    const finalStateRes = await fetch(`${baseUrl}/state`);
    const finalState = await finalStateRes.json();
    console.log("Final state:", finalState);

    // Print summary
    console.log("\nðŸ“ Test Summary");
    console.log("Initial Health Points:", initialState.healthPoints);
    console.log("Final Health Points:", finalState.healthPoints);
    console.log(
      "Total Mood Change:",
      finalState.healthPoints - initialState.healthPoints
    );
  } catch (error) {
    console.error("âŒ Test failed:", error);
    if (error.response) {
      console.error("Response status:", error.response.status);
      console.error("Response data:", await error.response.text());
    }
  }
}

console.log("ðŸ± Starting Cat Interaction Tests ðŸ±");
testPetInteractions().then(() => {
  console.log("\nâœ… Tests completed");
});

================
File: backend/tests/llm-test.ts
================
// backend/tests/llm-test.ts
import dotenv from "dotenv";
dotenv.config();

import { analyzePetInteraction } from "../services/llmService.js";

async function testLLM() {
  console.log("ðŸ” Testing LLM Service...");
  console.log("API Key present:", !!process.env.DEEPSEEK_API_KEY);

  const tests = [
    {
      type: "Positive",
      speech:
        "You're such a good kitty! I love you so much! You're the best cat ever!",
    },
    {
      type: "Negative",
      speech: "Bad cat! Get out of here! I don't want you around anymore!",
    },
    {
      type: "Neutral",
      speech: "Hey cat, what are you doing over there?",
    },
  ];

  for (const test of tests) {
    console.log(`\nðŸ—£ Testing ${test.type} Speech:`);
    console.log("Input:", test.speech);

    try {
      const result = await analyzePetInteraction(test.speech);
      console.log("âœ… Analysis Result:");
      console.log("Mood Impact:", result.moodImpact);
      console.log("Sentiment:", result.sentiment);
      console.log("Cat Reaction:", result.catReaction);
    } catch (error) {
      console.error("âŒ Test Failed:", error);
      if (error instanceof Error) {
        console.error("Error Details:", error.message);
        console.error("Stack:", error.stack);
      }
    }
  }
}

console.log("ðŸš€ Starting LLM Tests");
testLLM()
  .then(() => console.log("\nâœ¨ Tests Completed"))
  .catch((error) => console.error("âŒ Test Suite Failed:", error));

================
File: backend/.gitignore
================
# Create a comprehensive .gitignore in project root
cat > .gitignore << 'EOL'
# Dependencies
/node_modules
**/node_modules

# Python
**/__pycache__
*.py[cod]
*.$py.class

# Virtual Environment - Specific paths
/backend/ml/venv
/backend/ml/venv/
/backend/ml/venv/*
**/venv
**/.venv

# Build artifacts
/dist
/build
*.egg-info

# Environment files
.env
.env.local
.env.*

# IDE
.vscode
.idea
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Project specific
/backend/uploads

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
EOL

================
File: backend/index.ts
================
import dotenv from "dotenv";
dotenv.config();

import {connectToMongo} from "./startup/db.js";
import express from "express";
import setupRoutes from "./startup/routes.js";
import cors from "cors";

console.log(process.env.MONGO_URI);
console.log(process.env.FRONTEND_URL);
console.log(process.env.PORT);

const app = express();
const port = process.env.PORT || 4000;

// Enable CORS for frontend
app.use(
  cors({
    origin: process.env.FRONTEND_URL,
  })
);

// Connect to MongoDB
// connectToMongo();
app.use(express.json());

// Setup Routes
setupRoutes(app);

// Default Route
app.get("/", (req, res) => {
  res.send(`Hello, World!`);
});

// Start the Server
app.listen(port, () => {
  connectToMongo();
  console.log(`Server is running on http://localhost:${port}`);
});

================
File: backend/package.json
================
{
  "name": "backend",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "dev": "nodemon --watch . --ext ts --exec 'tsx --require dotenv/config index.ts'",
    "test:llm": "tsx tests/llm-test.ts"
  },
  "author": "",
  "license": "ISC",
  "description": "",
  "type": "module",
  "dependencies": {
    "@expo/vector-icons": "^14.0.0",
    "@react-native-async-storage/async-storage": "1.21.0",
    "@react-navigation/bottom-tabs": "^7.2.0",
    "@react-navigation/native": "^7.0.14",
    "@tensorflow/tfjs": "4.15.0",
    "@tensorflow/tfjs-converter": "4.15.0",
    "@tensorflow/tfjs-core": "4.15.0",
    "@tensorflow/tfjs-react-native": "1.0.0",
    "axios": "^1.7.9",
    "expo": "~52.0.0",
    "expo-blur": "~12.9.0",
    "expo-camera": "~13.6.0",
    "expo-constants": "~14.4.0",
    "expo-file-system": "~15.4.0",
    "expo-font": "~11.10.0",
    "expo-gl": "~13.0.0",
    "expo-gl-cpp": "~13.0.0",
    "expo-haptics": "~12.8.0",
    "expo-image-manipulator": "~11.8.0",
    "expo-image-picker": "~14.7.0",
    "expo-linking": "~5.0.0",
    "expo-router": "~2.0.0",
    "expo-splash-screen": "~0.26.0",
    "expo-status-bar": "~1.11.0",
    "expo-system-ui": "~2.9.0",
    "expo-web-browser": "~12.8.0",
    "react": "18.2.0",
    "react-dom": "18.2.0",
    "react-native": "0.73.2",
    "react-native-gesture-handler": "~2.14.0",
    "react-native-reanimated": "~3.6.0",
    "react-native-safe-area-context": "4.8.2",
    "react-native-screens": "~3.29.0",
    "react-native-web": "~0.19.6",
    "react-native-webview": "13.6.4"
  },
  "devDependencies": {
    "@babel/core": "^7.20.0",
    "@types/react": "~18.2.45",
    "typescript": "^5.1.3"
  }
}

================
File: backend/repomix-output.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-16T11:00:59.106Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
middleware/
  auth.ts
ml/
  src/
    api.py
    dataset.py
    model.py
    organize_dataset.py
    test_api.py
    test_model_webcam.py
    test_model.py
    train.py
  requirements.txt
models/
  user.ts
repos/
  foodRepo.ts
  stateRepo.ts
  userRepo.ts
routes/
  auth.ts
  food.ts
  ml.ts
  pet.ts
  state.ts
services/
  llmService.ts
startup/
  db.ts
  routes.ts
tests/
  llm-pet-test.js
  llm-test.ts
.gitignore
index.ts
package.json

================================================================
Files
================================================================

================
File: middleware/auth.ts
================
import jwt from "jsonwebtoken";

export const auth = (req, res, next) => {
  // Try to get token from cookies first (for web clients)
  let token = req.cookies?.jwt_token;

  // If not found, check Authorization header (for React Native)
  if (!token && req.headers.authorization) {
    const authHeader = req.headers.authorization;
    if (authHeader.startsWith("Bearer ")) {
      token = authHeader.split(" ")[1]; // Extract token from Bearer scheme
    }
  }

  if (!token) {
    return res.status(401).json({error: "Access denied. No token provided."});
  }

  const jwtPrivateKey = process.env.JWT_PRIVATE_KEY;
  if (!jwtPrivateKey) {
    return res.status(500).json({error: "JWT private key is not defined."});
  }

  try {
    const decoded = jwt.verify(token, jwtPrivateKey);
    req.user = decoded;
    next();
  } catch (ex) {
    res.status(400).json({error: "Invalid token."});
  }
};

================
File: ml/src/api.py
================
import sys
import json
import tensorflow as tf
import numpy as np
from PIL import Image

# Define categories
CATEGORIES = ["non_food", "food", "junk_food"]


def load_model():
    # Load the model from the relative path
    model_path = "../model/model_latest.h5"
    try:
        return tf.keras.models.load_model(model_path)
    except Exception as e:
        print(json.dumps({"error": f"Failed to load model: {str(e)}"}))
        sys.exit(1)


def preprocess_image(image_path):
    try:
        # Open and preprocess the image
        image = Image.open(image_path)
        image = image.resize((224, 224))
        image = np.array(image) / 255.0
        image = np.expand_dims(image, axis=0)
        return image
    except Exception as e:
        print(json.dumps({"error": f"Failed to process image: {str(e)}"}))
        sys.exit(1)


def main():
    if len(sys.argv) != 2:
        print(json.dumps({"error": "Image path not provided"}))
        sys.exit(1)

    image_path = sys.argv[1]

    # Load model
    model = load_model()

    # Process image
    processed_image = preprocess_image(image_path)

    try:
        # Make prediction
        predictions = model.predict(processed_image)[0]
        predicted_class = np.argmax(predictions)
        confidence = float(predictions[predicted_class])

        # Prepare response
        result = {
            "category": CATEGORIES[predicted_class],
            "confidence": confidence,
            "all_probabilities": {
                cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
            },
        }

        # Print result as JSON string
        print(json.dumps(result))

    except Exception as e:
        print(json.dumps({"error": f"Prediction failed: {str(e)}"}))
        sys.exit(1)


if __name__ == "__main__":
    main()

================
File: ml/src/dataset.py
================
# dataset.py
import tensorflow as tf
import os
import numpy as np
from typing import Tuple, Dict


class FoodDataset:
    def __init__(
        self,
        data_dir: str,
        img_size: Tuple[int, int] = (224, 224),
        batch_size: int = 32,
    ):
        """
        Initialize FoodDataset with enhanced preprocessing and validation

        Args:
            data_dir: Root directory of the dataset
            img_size: Target size for images (height, width)
            batch_size: Batch size for training
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.batch_size = batch_size
        self.categories = ["non_food", "healthy_food", "unhealthy_food"]

    def _parse_image(
        self, filename: tf.Tensor, label: tf.Tensor
    ) -> Tuple[tf.Tensor, tf.Tensor]:
        """
        Enhanced image parsing with robust augmentation and preprocessing
        """
        # Convert filename to string
        filename = tf.cast(filename, tf.string)
        label = tf.cast(label, tf.int32)

        # Read and decode image
        image = tf.io.read_file(filename)
        image = tf.image.decode_jpeg(image, channels=3)

        # Enhanced data augmentation pipeline
        image = tf.cast(image, tf.float32)

        # Random augmentations for training diversity
        image = tf.image.random_brightness(image, 0.2)
        image = tf.image.random_contrast(image, 0.8, 1.2)
        image = tf.image.random_saturation(image, 0.8, 1.2)
        image = tf.image.random_hue(image, 0.1)
        image = tf.image.random_flip_left_right(image)

        # 50% chance of additional augmentations
        if tf.random.uniform([]) > 0.5:
            image = tf.image.transpose(image)
            image = tf.image.random_flip_up_down(image)

        # Center crop before resize for consistent aspect ratio
        shape = tf.shape(image)
        min_dim = tf.minimum(shape[0], shape[1])
        image = tf.image.resize_with_crop_or_pad(image, min_dim, min_dim)

        # Resize to target size
        image = tf.image.resize(image, self.img_size)

        # Normalize pixel values
        image = tf.clip_by_value(image, 0.0, 255.0)
        image = image / 255.0

        # Convert label to one-hot encoding
        label = tf.one_hot(label, 3)

        return image, label

    def create_dataset(self, split: str = "training") -> tf.data.Dataset:
        """
        Create dataset with enhanced error handling and logging
        """
        split_dir = os.path.join(self.data_dir, split)

        # Initialize containers
        image_files = []
        labels = []
        category_counts = {category: 0 for category in self.categories}

        # Process each category
        for label, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)

            if not os.path.exists(category_dir):
                print(f"Warning: Directory not found: {category_dir}")
                continue

            # Get all valid image files
            valid_files = [
                os.path.join(category_dir, f)
                for f in os.listdir(category_dir)
                if f.lower().endswith((".jpg", ".jpeg", ".png"))
            ]

            category_counts[category] = len(valid_files)
            image_files.extend(valid_files)
            labels.extend([label] * len(valid_files))

        # Print dataset statistics
        print(f"\nDataset statistics for {split}:")
        print("-" * 50)
        for category, count in category_counts.items():
            print(f"{category}: {count} images")
        print(f"Total: {sum(category_counts.values())} images")

        # Verify dataset is not empty
        if not image_files:
            raise ValueError(f"No images found in {split_dir}")

        # Create TensorFlow dataset
        dataset = tf.data.Dataset.from_tensor_slices(
            (tf.constant(image_files), tf.constant(labels, dtype=tf.int32))
        )

        # Configure dataset for performance
        dataset = dataset.map(self._parse_image, num_parallel_calls=tf.data.AUTOTUNE)

        if split == "training":
            # Shuffle training data with larger buffer
            dataset = dataset.shuffle(
                buffer_size=min(50000, len(image_files)), reshuffle_each_iteration=True
            )

        # Optimize performance
        dataset = dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE).cache()

        return dataset

    def get_class_weights(self, split: str = "training") -> Dict[int, float]:
        """
        Calculate balanced class weights with improved handling of edge cases
        """
        split_dir = os.path.join(self.data_dir, split)

        # Count images in each class
        counts = {}
        for i, category in enumerate(self.categories):
            category_dir = os.path.join(split_dir, category)
            if os.path.exists(category_dir):
                counts[i] = len(
                    [
                        f
                        for f in os.listdir(category_dir)
                        if f.lower().endswith((".jpg", ".jpeg", ".png"))
                    ]
                )
            else:
                counts[i] = 0
                print(f"Warning: Directory not found: {category_dir}")

        # Calculate total samples
        total_samples = sum(counts.values())
        if total_samples == 0:
            raise ValueError(f"No images found in {split_dir}")

        # Calculate balanced weights
        n_classes = len(self.categories)
        weights = {}
        for class_idx, count in counts.items():
            if count == 0:
                weights[class_idx] = 1.0
            else:
                weights[class_idx] = total_samples / (n_classes * count)

        # Print weight distribution
        print(f"\nClass weights for {split}:")
        for i, category in enumerate(self.categories):
            print(f"{category}: {weights[i]:.4f}")

        return weights

    def validate_dataset(self) -> None:
        """
        Validate dataset integrity and balance
        """
        for split in ["training", "validation", "evaluation"]:
            split_dir = os.path.join(self.data_dir, split)
            if not os.path.exists(split_dir):
                print(f"Warning: Split directory not found: {split_dir}")
                continue

            total_images = 0
            corrupted_images = 0

            for category in self.categories:
                category_dir = os.path.join(split_dir, category)
                if not os.path.exists(category_dir):
                    print(f"Warning: Category directory not found: {category_dir}")
                    continue

                # Check each image
                for img_name in os.listdir(category_dir):
                    if not img_name.lower().endswith((".jpg", ".jpeg", ".png")):
                        continue

                    total_images += 1
                    img_path = os.path.join(category_dir, img_name)

                    try:
                        with tf.io.gfile.GFile(img_path, "rb") as fid:
                            image_data = fid.read()
                            _ = tf.image.decode_jpeg(image_data, channels=3)
                    except tf.errors.InvalidArgumentError:
                        print(f"Corrupted image found: {img_path}")
                        corrupted_images += 1

            print(f"\nDataset validation results for {split}:")
            print(f"Total images: {total_images}")
            print(f"Corrupted images: {corrupted_images}")

================
File: ml/src/model.py
================
import tensorflow as tf
from tensorflow.keras import layers, models


def create_model(input_shape=(224, 224, 3)):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze the base model
    base_model.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Changed to 3 outputs with softmax
        ]
    )

    # Compile the model
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",  # Use categorical_crossentropy for multi-class
        metrics=["accuracy"],
    )

    return model


def create_model_with_fine_tuning(input_shape=(224, 224, 3), fine_tune_layers=30):
    # Base model (MobileNetV2)
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape, include_top=False, weights="imagenet"
    )

    # Freeze early layers
    base_model.trainable = True
    for layer in base_model.layers[:-fine_tune_layers]:
        layer.trainable = False

    # Create new model with 3 output classes
    model = models.Sequential(
        [
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.2),
            layers.Dense(3, activation="softmax"),  # Three categories
        ]
    )

    # Compile with a lower learning rate for fine-tuning
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    return model


def get_model_summary(model):
    """Get model architecture summary"""
    trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.trainable_weights], axis=0)
    )
    non_trainable_params = tf.keras.backend.count_params(
        tf.concat([tf.reshape(w, [-1]) for w in model.non_trainable_weights], axis=0)
    )

    print("\nModel Summary:")
    print(f"Total parameters: {trainable_params + non_trainable_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")
    print(f"Non-trainable parameters: {non_trainable_params:,}")

    return {
        "total_params": trainable_params + non_trainable_params,
        "trainable_params": trainable_params,
        "non_trainable_params": non_trainable_params,
    }

================
File: ml/src/organize_dataset.py
================
# organize_dataset.py
import os
import shutil
from pathlib import Path

# Define healthy and unhealthy categories from Food-101 Data Set
HEALTHY_FOODS = [
    "beef_carpaccio",
    "beef_tartare",
    "beet_salad",
    "bibimbap",
    "caesar_salad",
    "caprese_salad",
    "ceviche",
    "chicken_curry",
    "edamame",
    "eggs_benedict",
    "falafel",
    "fried_rice",
    "gnocchi",
    "greek_salad",
    "grilled_salmon",
    "gyoza",
    "huevos_rancheros",
    "lasagna",
    "miso_soup",
    "mussels",
    "omelette",
    "pad_thai",
    "paella",
    "pancakes",
    "pho",
    "ramen",
    "risotto",
    "sashimi",
    "scallops",
    "seaweed_salad",
    "shrimp_and_grits",
    "spring_rolls",
    "steak",
    "sushi",
    "tuna_tartare",
    "waffles",
]

UNHEALTHY_FOODS = [
    "apple_pie",
    "baby_back_ribs",
    "baklava",
    "beignets",
    "bread_pudding",
    "breakfast_burrito",
    "cannoli",
    "carrot_cake",
    "cheesecake",
    "chicken_wings",
    "chocolate_cake",
    "chocolate_mousse",
    "churros",
    "croque_madame",
    "cup_cakes",
    "donuts",
    "french_fries",
    "fried_calamari",
    "hamburger",
    "hot_dog",
    "ice_cream",
    "macaroni_and_cheese",
    "macarons",
    "nachos",
    "onion_rings",
    "pizza",
    "poutine",
    "red_velvet_cake",
]


def create_hybrid_dataset():
    # Define paths and output to our hybrid_dataset (combination of the two)
    food101_path = "../data/food-101/images"
    food5k_path = "../data/Food-5k"
    output_path = "../data/hybrid_dataset"

    # Create directory structure
    for split in ["training", "validation", "evaluation"]:
        os.makedirs(os.path.join(output_path, split, "healthy_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "non_food"), exist_ok=True)
        os.makedirs(os.path.join(output_path, split, "unhealthy_food"), exist_ok=True)

    # Copy non-food images from Food-5K
    for split in ["training", "validation", "evaluation"]:
        src_dir = os.path.join(food5k_path, split, "non_food")
        dst_dir = os.path.join(output_path, split, "non_food")

        print(f"Copying non-food images from {split} set...")
        for img in os.listdir(src_dir):
            if img.endswith(".jpg"):
                shutil.copy2(os.path.join(src_dir, img), os.path.join(dst_dir, img))

    # Copy and categorize food images from Food-101
    total_images_per_category = 1000  # Adjust this number as needed

    print("\nCopying healthy food images from Food-101...")
    for category in HEALTHY_FOODS:
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        # Calculate split sizes
        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        # Split images
        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in healthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "healthy_food", f"{category}_{img}"
                    ),
                )

    print("\nCopying unhealthy food images from Food-101...")
    for category in UNHEALTHY_FOODS:
        # Same process for unhealthy foods
        category_path = os.path.join(food101_path, category)
        if not os.path.exists(category_path):
            print(f"Warning: Category {category} not found")
            continue

        images = [f for f in os.listdir(category_path) if f.endswith(".jpg")]
        images = images[:total_images_per_category]

        num_images = len(images)
        train_size = int(num_images * 0.7)
        val_size = int(num_images * 0.15)

        train_images = images[:train_size]
        val_images = images[train_size : train_size + val_size]
        test_images = images[train_size + val_size :]

        # Copy to respective splits in unhealthy_food directory
        for split_info in [
            ("training", train_images),
            ("validation", val_images),
            ("evaluation", test_images),
        ]:
            split_name, split_images = split_info
            for img in split_images:
                shutil.copy2(
                    os.path.join(category_path, img),
                    os.path.join(
                        output_path, split_name, "unhealthy_food", f"{category}_{img}"
                    ),
                )

    # Print final dataset statistics
    print("\nFinal Dataset Statistics:")
    for split in ["training", "validation", "evaluation"]:
        healthy_count = len(
            os.listdir(os.path.join(output_path, split, "healthy_food"))
        )
        non_food_count = len(os.listdir(os.path.join(output_path, split, "non_food")))
        unhealthy_count = len(
            os.listdir(os.path.join(output_path, split, "unhealthy_food"))
        )
        print(f"\n{split.capitalize()} set:")
        print(f"Healthy food images: {healthy_count}")
        print(f"Non-food images: {non_food_count}")
        print(f"Unhealthy food images: {unhealthy_count}")
        print(f"Total: {healthy_count + non_food_count + unhealthy_count}")


if __name__ == "__main__":
    print("Creating hybrid dataset...")
    create_hybrid_dataset()
    print("\nDone!")

================
File: ml/src/test_api.py
================
# test_api.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os


def load_and_preprocess_image(image_path):
    """Load and preprocess a single image"""
    img = Image.open(image_path)
    img = img.resize((224, 224))
    img_array = np.array(img) / 255.0
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_images():
    """Test model predictions on test images"""
    # Categories
    CATEGORIES = ["non_food", "food", "junk_food"]

    # Load model
    print("Loading model...")
    model_path = "../model/model_latest.h5"
    try:
        model = tf.keras.models.load_model(model_path)
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    # Test directory
    test_dir = "../test_images"
    test_images = sorted(
        [f for f in os.listdir(test_dir) if f.endswith((".jpg", ".jpeg"))]
    )

    print("\nTesting images...")
    print("=" * 50)

    results = []
    for image_name in test_images:
        image_path = os.path.join(test_dir, image_name)
        try:
            # Process image
            img_array = load_and_preprocess_image(image_path)

            # Get predictions
            predictions = model.predict(img_array, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Store results
            result = {
                "image": image_name,
                "predicted": CATEGORIES[predicted_class],
                "confidence": confidence,
                "probabilities": {
                    cat: float(prob) for cat, prob in zip(CATEGORIES, predictions)
                },
            }
            results.append(result)

            # Print results
            print(f"\nImage: {image_name}")
            print(f"Predicted: {result['predicted']}")
            print(f"Confidence: {confidence:.4f}")
            print("Probabilities:")
            for cat, prob in result["probabilities"].items():
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

        except Exception as e:
            print(f"Error processing {image_name}: {e}")

    # Print summary
    print("\nSummary:")
    for category in CATEGORIES:
        count = sum(1 for r in results if r["predicted"] == category)
        print(f"{category}: {count} images")


if __name__ == "__main__":
    test_images()

================
File: ml/src/test_model_webcam.py
================
import tensorflow as tf
import cv2
import numpy as np
import os
import sys


def load_and_preprocess_frame(frame):
    resized = cv2.resize(frame, (224, 224))
    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)
    normalized = rgb / 255.0
    batched = np.expand_dims(normalized, 0)
    return batched


def main():
    # Load the trained model
    print("Loading model...")
    model_path = "../model_latest.h5"
    if not os.path.exists(model_path):
        print(f"Error: Model not found at {model_path}")
        sys.exit(1)

    model = tf.keras.models.load_model(model_path)

    # Category labels and colors
    CATEGORIES = ["NOT FOOD", "HEALTHY FOOD", "UNHEALTHY FOOD"]
    COLORS = [
        (0, 0, 255),  # Red for NOT FOOD
        (0, 255, 0),  # Green for HEALTHY FOOD
        (0, 165, 255),  # Orange for UNHEALTHY FOOD (BGR format)
    ]

    print("Starting webcam...")
    cap = cv2.VideoCapture(0)

    # Check if camera opened successfully
    if not cap.isOpened():
        print("\nError: Could not open camera")
        print(
            "Please check your camera permissions in System Settings > Privacy & Security > Camera"
        )
        print("Make sure Terminal/Python has permission to access the camera")
        sys.exit(1)

    print("Camera accessed successfully!")
    print("Press 'q' to quit")

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("Failed to grab frame")
                break

            processed_frame = load_and_preprocess_frame(frame)
            predictions = model.predict(processed_frame, verbose=0)[0]
            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            # Get all probabilities
            probabilities = {cat: prob for cat, prob in zip(CATEGORIES, predictions)}

            # Prepare text display
            text = CATEGORIES[predicted_class]
            confidence_text = f"Confidence: {confidence:.2f}"

            # Additional probabilities text
            prob_texts = [f"{cat}: {prob:.2f}" for cat, prob in probabilities.items()]

            # Background rectangle for text (make it larger for all probabilities)
            cv2.rectangle(frame, (10, 10), (300, 110), (0, 0, 0), -1)

            # Add main prediction and confidence
            cv2.putText(
                frame,
                text,
                (20, 35),
                cv2.FONT_HERSHEY_SIMPLEX,
                1,
                COLORS[predicted_class],
                2,
            )
            cv2.putText(
                frame,
                confidence_text,
                (20, 60),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.7,
                (255, 255, 255),
                2,
            )

            # Add all probabilities
            y_offset = 85
            cv2.putText(
                frame,
                f"All: {' | '.join(prob_texts)}",
                (20, y_offset),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.4,
                (255, 255, 255),
                1,
            )

            cv2.imshow("Food Detector", frame)

            if cv2.waitKey(1) & 0xFF == ord("q"):
                print("\nQuitting...")
                break

    except KeyboardInterrupt:
        print("\nInterrupted by user")
    except Exception as e:
        print(f"\nAn error occurred: {str(e)}")
    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == "__main__":
    main()

================
File: ml/src/test_model.py
================
# test_model.py
import tensorflow as tf
import numpy as np
from PIL import Image
import os
import random


def load_and_preprocess_image(image_path):
    # Load image
    img = Image.open(image_path)
    # Resize to match model's expected input
    img = img.resize((224, 224))
    # Convert to array and normalize
    img_array = np.array(img) / 255.0
    # Add batch dimension
    img_array = np.expand_dims(img_array, 0)
    return img_array


def test_model():
    # Load the trained model
    print("Loading model...")
    model = tf.keras.models.load_model("../model_final.h5")

    # Print model summary
    print("\nModel Summary:")
    model.summary()

    # Paths for evaluation set
    eval_dir = "../data/hybrid_dataset/evaluation"
    healthy_food_dir = os.path.join(eval_dir, "healthy_food")
    non_food_dir = os.path.join(eval_dir, "non_food")
    unhealthy_food_dir = os.path.join(eval_dir, "unhealthy_food")

    # Get 5 random images from each category
    healthy_food_images = random.sample(
        [f for f in os.listdir(healthy_food_dir) if f.endswith(".jpg")], 5
    )
    non_food_images = random.sample(
        [f for f in os.listdir(non_food_dir) if f.endswith(".jpg")], 5
    )
    unhealthy_food_images = random.sample(
        [f for f in os.listdir(unhealthy_food_dir) if f.endswith(".jpg")], 5
    )

    categories = ["Non-Food", "Healthy Food", "Unhealthy Food"]

    def test_category(images, directory, category_name):
        print(f"\nTesting {category_name} Images:")
        print("=" * 50)
        for img_name in images:
            img_path = os.path.join(directory, img_name)
            img_array = load_and_preprocess_image(img_path)
            predictions = model.predict(img_array, verbose=0)[0]

            predicted_class = np.argmax(predictions)
            confidence = predictions[predicted_class]

            print(f"\nImage: {img_name}")
            print(f"Prediction: {categories[predicted_class]}")
            print(f"Confidence: {confidence:.4f}")
            print("All probabilities:")
            for cat, prob in zip(categories, predictions):
                print(f"  {cat}: {prob:.4f}")
            print("-" * 50)

    # Test all categories
    test_category(healthy_food_images, healthy_food_dir, "Healthy Food")
    test_category(non_food_images, non_food_dir, "Non-Food")
    test_category(unhealthy_food_images, unhealthy_food_dir, "Unhealthy Food")


if __name__ == "__main__":
    print("Testing model on all three categories...")
    test_model()

================
File: ml/src/train.py
================
# train.py
import tensorflow as tf
from tensorflow.keras import layers, models
from dataset import FoodDataset
import os
import shutil


def ensure_directories():
    """Create necessary directories for model saving"""
    # Create both src/checkpoints and checkpoints directories
    directories = ["checkpoints", "src/checkpoints"]
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"Created directory: {directory}")


def save_model_with_verification(model, base_path, filename):
    """Save model and verify it exists in both locations"""
    # Save to primary location
    primary_path = os.path.join(base_path, filename)
    model.save(primary_path)
    print(f"Model saved to: {primary_path}")

    # Copy to src/checkpoints for compatibility
    src_path = os.path.join("src/checkpoints", filename)
    shutil.copy2(primary_path, src_path)
    print(f"Model copied to: {src_path}")

    return primary_path, src_path


def train():
    # Configuration
    DATA_DIR = "../data/hybrid_dataset"
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    EPOCHS = 15

    # Ensure directories exist
    ensure_directories()

    print(f"Loading data from: {os.path.abspath(DATA_DIR)}")

    # Create datasets
    dataset = FoodDataset(DATA_DIR, IMG_SIZE, BATCH_SIZE)
    train_ds = dataset.create_dataset("training")
    val_ds = dataset.create_dataset("validation")

    # Get class weights
    class_weights = dataset.get_class_weights("training")
    print("\nClass weights:", class_weights)

    # Create model
    print("\nCreating model...")
    model = models.Sequential(
        [
            tf.keras.applications.MobileNetV2(
                input_shape=(*IMG_SIZE, 3), include_top=False, weights="imagenet"
            ),
            layers.GlobalAveragePooling2D(),
            layers.Dense(256, activation="relu"),
            layers.Dropout(0.4),
            layers.Dense(128, activation="relu"),
            layers.Dropout(0.3),
            layers.Dense(3, activation="softmax"),
        ]
    )

    # Compile model
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    # Callbacks
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            filepath="checkpoints/best_model.h5",
            save_best_only=True,
            monitor="val_accuracy",
            mode="max",
            verbose=1,
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor="val_accuracy", patience=5, restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.2, patience=3, min_lr=1e-6
        ),
    ]

    # Train model
    print("\nStarting training...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=callbacks,
        class_weight=class_weights,
        verbose=1,
    )

    # Save final model with verification
    print("\nSaving final model...")
    try:
        primary_path, src_path = save_model_with_verification(
            model, "checkpoints", "model.h5"
        )
        print(f"Model successfully saved and verified at:")
        print(f"1. {primary_path}")
        print(f"2. {src_path}")
    except Exception as e:
        print(f"Error saving model: {str(e)}")

    return history, model


if __name__ == "__main__":
    print("Starting training process...")
    try:
        history, model = train()

        # Verify saved model exists
        expected_paths = ["checkpoints/model.h5", "src/checkpoints/model.h5"]
        for path in expected_paths:
            if os.path.exists(path):
                print(f"Verified: Model exists at {path}")
            else:
                print(f"Warning: Model not found at {path}")

    except Exception as e:
        print(f"Error during training: {str(e)}")

================
File: ml/requirements.txt
================
asgiref==3.5.2
certifi==2022.9.24
charset-normalizer==2.1.1
click==8.1.3
confluent-kafka==2.4.0
contourpy==1.0.7
cycler==0.11.0
Django==4.1.3
djangorestframework==3.14.0
Flask==1.1.2
Flask-Login==0.5.0
Flask-SQLAlchemy==2.4.4
fonttools==4.39.4
future==1.0.0
greenlet==2.0.1
idna==3.4
iso8601==2.1.0
itsdangerous==2.1.2
Jinja2==3.1.2
khan-api-wrapper==0.0.18
kiwisolver==1.4.4
MarkupSafe==2.1.1
matplotlib==3.7.1
mpmath==1.2.1
numpy==1.23.5
packaging==23.1
pandas==2.0.2
Pillow==9.3.0
pybraille==1.0.0
pygame==2.1.2
pyparsing==3.0.9
pyserial==3.5
python-dateutil==2.8.2
pytube==12.1.2
pytz==2022.6
PyYAML==6.0.2
rauth==0.7.3
requests==2.28.1
scipy==1.9.3
serial==0.0.97
six==1.16.0
SQLAlchemy==1.4.44
sqlparse==0.4.3
sympy==1.11.1
tzdata==2023.3
urllib3==1.26.12
Werkzeug==2.2.2
tensorflow==2.11.0

================
File: models/user.ts
================
import Joi from "joi";
import mongoose from "mongoose";
import jwt from "jsonwebtoken";

interface IUserDocument extends mongoose.Document {
  firstName: string;
  lastName: string;
  email: string;
  password: string;
  petName: string;
  healthPoints: number;
  mood: "happy" | "sad" | "neutral";
  generateAuthToken(): string;
}

const userSchema = new mongoose.Schema<IUserDocument>({
  firstName: {
    type: String,
    required: true,
    maxlength: 50,
  },
  lastName: {
    type: String,
    required: true,
    maxlength: 50,
  },
  email: {
    type: String,
    required: true,
    minlength: 5,
    maxlength: 255,
    unique: true,
  },
  password: {
    type: String,
    required: true,
    minlength: 5,
    maxlength: 1024,
  },
  petName: {
    type: String,
    required: true,
    minlength: 1,
    maxlength: 255,
  },
  healthPoints: {
    type: Number,
    default: 100,
  },
  mood: {
    type: String,
    enum: ["happy", "sad", "neutral"],
    default: "neutral",
  },
});

userSchema.methods.generateAuthToken = function () {
  const token = jwt.sign(
    {
      _id: this._id,
      firstName: this.firstName,
      lastName: this.lastName,
      role: "user",
    },
    process.env.JWT_PRIVATE_KEY || "this is a secret key"
  );
  return token;
};

export const User = mongoose.model("users", userSchema);

export function validateUser(user) {
  const schema = Joi.object({
    firstName: Joi.string().max(50).required(),
    lastName: Joi.string().max(50).required(),
    email: Joi.string().min(5).max(255).required().email(),
    password: Joi.string().min(5).max(255).required(),
    petName: Joi.string().min(1).max(255).required(),
  });
  return schema.validate(user);
}

================
File: repos/foodRepo.ts
================
import multer, { FileFilterCallback } from "multer";
import path from "path";
import fs from "fs";
import { Request } from "express";
import { fileURLToPath } from "url";

// Types
interface MulterFile {
  fieldname: string;
  originalname: string;
  encoding: string;
  mimetype: string;
  size: number;
  destination: string;
  filename: string;
  path: string;
  buffer: Buffer;
}

// Fix for __dirname in ES modules
const currentFilePath = fileURLToPath(import.meta.url);
const currentDirPath = path.dirname(currentFilePath);

// Create uploads directory path
const uploadPath = path.join(currentDirPath, "..", "ml", "uploads");

// Ensure uploads directory exists
if (!fs.existsSync(uploadPath)) {
  fs.mkdirSync(uploadPath, { recursive: true });
  console.log(`Created uploads directory at: ${uploadPath}`);
}

const storage = multer.diskStorage({
  destination: function (
    _req: Request,
    _file: MulterFile,
    cb: (error: Error | null, destination: string) => void
  ) {
    cb(null, uploadPath);
  },
  filename: function (
    _req: Request,
    file: MulterFile,
    cb: (error: Error | null, filename: string) => void
  ) {
    const uniqueSuffix = Date.now() + "-" + Math.round(Math.random() * 1e9);
    cb(null, uniqueSuffix + "-" + file.originalname);
  },
});

// Add file filter for images
const fileFilter = (
  _req: Request,
  file: MulterFile,
  cb: FileFilterCallback
) => {
  const allowedMimes = ["image/jpeg", "image/png", "image/jpg"];
  if (allowedMimes.includes(file.mimetype)) {
    cb(null, true);
  } else {
    cb(new Error("Invalid file type. Only JPEG and PNG are allowed."));
  }
};

// Create multer instance with configuration
const upload = multer({
  storage,
  fileFilter,
  limits: {
    fileSize: 50 * 1024 * 1024, // 5MB limit
  },
});

export const singleFoodUpload = upload.single("photo");

export async function processFoodFile(file: MulterFile) {
  const result = await fetch("http://localhost:4000/api/ml/classify", {
    method: "POST",
    body: JSON.stringify({
      argument: file.path
    }),
    headers: {
      "Content-Type": "application/json"
    }
  });

  const data = await result.json();

  return {
    data
  };
}

================
File: repos/stateRepo.ts
================
import {User} from "../models/user";

export const getStates = async (req, res) => {
  const userId = process.env.DEMO_USER_ID;
  const user = await User.findById(userId);
  if (!user) return res.status(404).json({error: "User not found."});
  return res.status(200).json({HP: user.healthPoints, mood: user.mood});
};

================
File: repos/userRepo.ts
================
import {User, validateUser} from "../models/user.js";
import Joi from "joi";
import bcrypt from "bcryptjs";
const {hash, genSalt, compare} = bcrypt;
import _ from "lodash";

export const login = async (req, res) => {
  const {error} = validate(req.body);
  if (error) return res.status(400).json({error: error.details[0].message});

  let user = await User.findOne({email: req.body.email});
  if (!user) return res.status(400).json({error: "Invalid email or password."});

  if (!user.password)
    return res.status(400).json({error: "Invalid email or password."});

  const validPassword = await compare(req.body.password, user.password);
  if (!validPassword)
    return res.status(400).json({error: "Invalid email or password."});

  const token = user.generateAuthToken();

  res.status(200).json({
    message: "Login successful",
    token,
    user: {
      _id: user._id,
      email: user.email,
      firstName: user.firstName,
      lastName: user.lastName,
      petName: user.petName,
      healthPoints: user.healthPoints,
      mood: user.mood,
    },
  });
};

export const register = async (req, res) => {
  const {error} = validateUser(req.body);
  if (error) return res.status(400).send(error.details[0].message);

  let user = await User.findOne({email: req.body.email});
  if (user) return res.status(400).send("User already registered.");

  user = new User(
    _.pick(req.body, ["firstName", "lastName", "email", "password", "petName"])
  );
  const salt = await genSalt(10);
  user.password = await hash(user.password, salt);
  await user.save();

  const token = user.generateAuthToken();

  return res.status(201).json({
    message: "Registration successful",
    token,
    user: _.pick(user, [
      "_id",
      "firstName",
      "lastName",
      "email",
      "petName",
      "healthPoints",
      "mood",
    ]),
  });
};

export const getMe = async (req, res) => {
  const user = await User.findById(req.user._id).select("-password");
  return res.status(200).json({
    user: _.pick(user, [
      "_id",
      "firstName",
      "lastName",
      "email",
      "petName",
      "healthPoints",
      "mood",
    ]),
  });
};

function validate(req) {
  const schema = {
    email: Joi.string().min(5).max(255).required().email(),
    password: Joi.string().min(5).max(255).required(),
  };

  return Joi.object(schema).validate(req.body);
}

export const test = (req, res) => {
  return res.status(200).json({message: "Test route"});
};

================
File: routes/auth.ts
================
import {auth} from "./../middleware/auth";
import express from "express";
import {getMe, login, register, test} from "../repos/userRepo";

const router = express.Router();

router.post("/", login);
router.post("/register", register);
router.get("/test", auth, test);
router.get("/me", auth, getMe);

export default router;

================
File: routes/food.ts
================
import { Router, Request, Response } from "express";
import { singleFoodUpload, processFoodFile } from "../repos/foodRepo.js";

const router = Router();

// POST /food
router.post("/", singleFoodUpload, (req: Request, res: Response) => {
  try {
    console.log("Received request with file:", req.file);

    if (!req.file) {
      console.log("No file in request");
      return res.status(400).json({ error: "No file uploaded." });
    }

    // Process the uploaded file
    console.log("Processing file...");
    console.log("File size:", req.file.size);
    console.log("File path:", req.file);
    const processedFile = processFoodFile(req.file);
    console.log("Processed file result:", processedFile);

    return res.json(processedFile);
  } catch (error) {
    console.error("Error in /food route:", error);
    return res.status(500).json({
      error: "Internal server error.",
      details: error instanceof Error ? error.message : "Unknown error",
    });
  }
});

export default router;

================
File: routes/ml.ts
================
// import express, { Request, Response } from "express";
import { spawn } from "child_process";
import path from "path";
// import multer from "multer";
// import { fileURLToPath } from "url";
// import fs from "fs/promises";

// // Types
// interface ClassificationResult {
//   category: string;
//   confidence: number;
//   all_probabilities: {
//     [key: string]: number;
//   };
// }

// interface ErrorResponse {
//   error: string;
//   details?: unknown;
// }

// // Configuration
// const __filename = fileURLToPath(import.meta.url);
// const __dirname = path.dirname(__filename);

// // Multer configuration with file filtering and size limits
// const storage = multer.diskStorage({
//   destination: "uploads/",
//   filename: (req, file, cb) => {
//     const uniqueSuffix = `${Date.now()}-${Math.round(Math.random() * 1e9)}`;
//     cb(null, `${uniqueSuffix}-${file.originalname}`);
//   },
// });

// const fileFilter = (
//   req: Express.Request,
//   file: Express.Multer.File,
//   cb: multer.FileFilterCallback
// ) => {
//   const allowedTypes = ["image/jpeg", "image/png", "image/jpg"];

//   if (allowedTypes.includes(file.mimetype)) {
//     cb(null, true);
//   } else {
//     cb(new Error("Invalid file type. Only JPEG and PNG are allowed."));
//   }
// };

// const upload = multer({
//   storage,
//   fileFilter,
//   limits: {
//     fileSize: 5 * 1024 * 1024, // 5MB limit
//   },
// });

// const router = express.Router();

// // Helper function to run Python script
// const runPythonScript = async (
//   scriptPath: string,
//   args: string[] = []
// ): Promise<string> => {
//   return new Promise((resolve, reject) => {
//     const pythonProcess = spawn("python", [scriptPath, ...args]);
//     let result = "";
//     let errorOutput = "";

//     pythonProcess.stdout.on("data", (data) => {
//       result += data.toString();
//     });

//     pythonProcess.stderr.on("data", (data) => {
//       errorOutput += data.toString();
//     });

//     pythonProcess.on("close", (code) => {
//       if (code === 0) {
//         resolve(result);
//       } else {
//         reject(
//           new Error(`Python script failed with code ${code}: ${errorOutput}`)
//         );
//       }
//     });

//     pythonProcess.on("error", (error) => {
//       reject(error);
//     });
//   });
// };

// // Cleanup function for uploaded files
// const cleanupFile = async (filePath: string): Promise<void> => {
//   try {
//     await fs.unlink(filePath);
//   } catch (error) {
//     console.error(`Failed to cleanup file ${filePath}:`, error);
//   }
// };

// // Routes
// router.post(
//   "/predict",
//   upload.single("image"),
//   async (
//     req: Request,
//     res: Response<ClassificationResult | ErrorResponse>
//   ): Promise<void> => {
//     if (!req.file) {
//       res.status(400).json({ error: "No image provided" });
//       return;
//     }

//     try {
//       const result = await runPythonScript(
//         path.join(__dirname, "../ml/src/api.py"),
//         [req.file.path]
//       );

//       const classification: ClassificationResult = JSON.parse(result);
//       res.json(classification);
//     } catch (error) {
//       console.error("Classification error:", error);
//       res.status(500).json({
//         error: "Classification failed",
//         details: error instanceof Error ? error.message : String(error),
//       });
//     } finally {
//       // Cleanup uploaded file
//       if (req.file) {
//         await cleanupFile(req.file.path);
//       }
//     }
//   }
// );

// router.get(
//   "/training-info",
//   async (req: Request, res: Response): Promise<void> => {
//     try {
//       const result = await runPythonScript(
//         path.join(__dirname, "../ml/src/test_api.py")
//       );

//       const info = JSON.parse(result);
//       res.json(info);
//     } catch (error) {
//       console.error("Training info error:", error);
//       res.status(500).json({
//         error: "Could not get training info",
//         details: error instanceof Error ? error.message : String(error),
//       });
//     }
//   }
// );

// // Error handling middleware
// router.use(
//   (error: Error, req: Request, res: Response, next: express.NextFunction) => {
//     console.error("ML Router Error:", error);

//     if (error instanceof multer.MulterError) {
//       if (error.code === "LIMIT_FILE_SIZE") {
//         res
//           .status(400)
//           .json({ error: "File size is too large. Maximum size is 5MB." });
//       } else {
//         res.status(400).json({ error: error.message });
//       }
//     } else {
//       res.status(500).json({ error: "Internal server error" });
//     }
//   }
// );

// export default router;


// ... existing code ...

import { Router, Request, Response } from "express";

const router = Router();

const __dirname = path.resolve();

// Routes
router.post("/classify", async (req: Request, res: Response): Promise<void> => {
  const { argument } = req.body;
  
  if (!argument) {
    res.status(400).json({ error: "No argument provided" });
    return;
  }

  try {
    console.log("Running Python script with argument:", argument);
    const result = await new Promise<string>((resolve, reject) => {
      const pythonProcess = spawn("python3", [`${__dirname}/ml/src/test_api.py`, argument]);
      let result = "";
      let errorOutput = "";

      pythonProcess.stdout.on("data", (data) => {
        result += data.toString();
      });

      pythonProcess.stderr.on("data", (data) => {
        errorOutput += data.toString();
      });

      pythonProcess.on("close", (code) => {
        if (code === 0) {
          resolve(result);
        } else {
          reject(new Error(`Python script failed with code ${code}: ${errorOutput}`));
        }
      });

      pythonProcess.on("error", (error) => {
        reject(error);
      });
    });

    const classification = JSON.parse(result);
    res.json(classification);
  } catch (error) {
    console.error("Script execution error:", error);
    res.status(500).json({
      error: "Script execution failed",
      details: error instanceof Error ? error.message : String(error),
    });
  }
});

export default router;
// ... existing code ...

================
File: routes/pet.ts
================
// backend/routes/pet.ts
import express, { Request, Response } from "express";
import { analyzePetInteraction } from "../services/llmService";
import { User } from "../models/user";

const router = express.Router();

const DEMO_USER_ID = process.env.DEMO_USER_ID;

const getDemoUser = async () => {
  let user = await User.findById(DEMO_USER_ID);
  if (!user) {
    // Create demo user if none exists
    user = await User.create({
      _id: DEMO_USER_ID, // Use the specific ID
      firstName: "Demo",
      lastName: "User",
      email: "demo@example.com",
      password: "demo123", // In a real app, this should be hashed
      petName: "Kitty",
      healthPoints: 100,
      mood: "neutral",
    });
  }
  return user;
};

router.post("/interact", (req: Request, res: Response) => {
  (async () => {
    try {
      const { speech } = req.body;
      if (!speech) {
        return res.status(400).json({ error: "Speech content is required" });
      }

      // Analyze speech with LLM
      const analysis = await analyzePetInteraction(speech);

      // Get demo user
      const user = await getDemoUser();

      // Calculate new health points
      const newHP = Math.max(
        0,
        Math.min(100, user.healthPoints + analysis.moodImpact)
      );

      // Determine new mood based on health points
      let newMood: "happy" | "sad" | "neutral";
      if (newHP >= 70) {
        newMood = "happy";
      } else if (newHP <= 30) {
        newMood = "sad";
      } else {
        newMood = "neutral";
      }

      // Update user's pet state
      await User.findByIdAndUpdate(user._id, {
        healthPoints: newHP,
        mood: newMood,
      });

      // Return response
      return res.json({
        petState: {
          healthPoints: newHP,
          mood: newMood,
          petName: user.petName,
        },
        analysis,
      });
    } catch (error) {
      return res.status(500).json({
        error: "Failed to process pet interaction",
        details: error instanceof Error ? error.message : String(error),
      });
    }
  })();
});

router.get("/state", (req: Request, res: Response) => {
  (async () => {
    try {
      const user = await getDemoUser();

      return res.json({
        petName: user.petName,
        healthPoints: user.healthPoints,
        mood: user.mood,
      });
    } catch (error) {
      return res.status(500).json({
        error: "Failed to get pet state",
        details: error instanceof Error ? error.message : String(error),
      });
    }
  })();
});

export default router;

================
File: routes/state.ts
================
import {auth} from "./../middleware/auth";
import express from "express";
import {login, register, test} from "../repos/userRepo";
import { getStates } from "../repos/stateRepo";

const router = express.Router();

router.get("/", getStates);

export default router;

================
File: services/llmService.ts
================
// backend/services/llmService.ts
import OpenAI from "openai";
import dotenv from "dotenv";
dotenv.config();

if (!process.env.DEEPSEEK_API_KEY) {
  throw new Error("DEEPSEEK_API_KEY is not set in environment variables");
}

const openai = new OpenAI({
  apiKey: process.env.DEEPSEEK_API_KEY,
  baseURL: "https://api.deepseek.com",
});

interface PetAnalysis {
  moodImpact: number;
  sentiment: "positive" | "negative" | "neutral";
  catReaction: string;
}

export async function analyzePetInteraction(
  speech: string
): Promise<PetAnalysis> {
  try {
    console.log("ðŸ“ Processing speech:", speech);

    const response = await openai.chat.completions.create({
      model: "deepseek-chat",
      messages: [
        {
          role: "system",
          content: `You are an emotion analyzer for a virtual pet cat. Analyze the speech and return ONLY a JSON object.
          
For example:
For positive speech: {"moodImpact": 7, "sentiment": "positive", "catReaction": "Purrs loudly and rubs against your leg"}
For negative speech: {"moodImpact": -5, "sentiment": "negative", "catReaction": "Flattens ears and backs away"}
For neutral speech: {"moodImpact": 0, "sentiment": "neutral", "catReaction": "Glances briefly and continues current activity"}

Rules:
1. Positive speech (praise, love) = positive impact (1 to 10)
2. Negative speech (scolding, threats) = negative impact (-1 to -10)
3. Stronger emotions = stronger impact
4. Cat reactions must be realistic

Return only the JSON object, no additional text or formatting.`,
        },
        { role: "user", content: speech },
      ],
      temperature: 0.7,
      max_tokens: 150,
    });

    console.log("ðŸ¤– Raw LLM Response:", response.choices[0].message.content);

    const content = response.choices[0].message.content;
    if (!content) {
      throw new Error("Empty response from LLM");
    }

    // Clean up the response if needed
    const cleanContent = content
      .replace(/```json\n?/, "")
      .replace(/```\n?/, "")
      .trim();

    try {
      const analysis = JSON.parse(cleanContent);

      // Validate the response
      if (
        typeof analysis.moodImpact !== "number" ||
        !["positive", "negative", "neutral"].includes(analysis.sentiment) ||
        typeof analysis.catReaction !== "string"
      ) {
        throw new Error("Invalid response format from LLM");
      }

      return analysis;
    } catch (parseError) {
      console.error("Parse error:", parseError);
      console.error("Content that failed to parse:", cleanContent);
      throw new Error("Failed to parse LLM response");
    }
  } catch (error) {
    console.error("ðŸš¨ LLM Analysis failed:", error);
    // Return a fallback response instead of throwing
    return {
      moodImpact: 0,
      sentiment: "neutral",
      catReaction: "The cat seems unsure how to react.",
    };
  }
}

================
File: startup/db.ts
================
import mongoose from "mongoose";

const uri = process.env.MONGO_URI;

if (!uri) {
  console.error("MONGODB_URI is not defined.");
  process.exit(1);
}

export async function connectToMongo() {
  try {
    await mongoose.connect(uri || "");
    console.log("Connected to MongoDB using Mongoose");
  } catch (err) {
    console.error("Error connecting to MongoDB with Mongoose:", err);
  }
}

================
File: startup/routes.ts
================
import authRoutes from "../routes/auth.js";
import stateRoutes from "../routes/state.js";
import petRoutes from "../routes/pet.js";
import foodRoutes from "../routes/food.js";
import mlRoutes from "../routes/ml.js";
const setupRoutes = (app) => {
  app.use("/api/auth", authRoutes);
  app.use("/api/state", stateRoutes);
  app.use("/api/pet", petRoutes);
  app.use("/api/food", foodRoutes);
  app.use("/api/ml", mlRoutes);
};

export default setupRoutes;

================
File: tests/llm-pet-test.js
================
// backend/tests/llm-pet-test.js
import fetch from "node-fetch";

const positiveSpeeches = [
  {
    description: "Loving and affectionate",
    speech:
      "You're the most precious kitty in the world! I love you so much and I'll always take care of you. You make me so happy every single day!",
  },
  {
    description: "Praising good behavior",
    speech:
      "What a good kitty you are! You're so well-behaved and gentle. I'm so proud of how you always use your scratching post!",
  },
  {
    description: "Gentle and soothing",
    speech:
      "Hey sweet baby, you're such a calm and beautiful cat. Your purring makes me feel so peaceful and loved.",
  },
];

const negativeSpeeches = [
  {
    description: "Harsh scolding",
    speech:
      "You're such a terrible cat! I can't believe you knocked everything off the table again! I regret getting you!",
  },
  {
    description: "Threatening",
    speech:
      "If you don't stop scratching the furniture, I'm going to get rid of you! You're the worst pet ever!",
  },
  {
    description: "Cold and dismissive",
    speech:
      "Just go away, I don't want you around. You're nothing but trouble and I wish I never got a cat.",
  },
];

async function testPetInteractions() {
  const baseUrl = "http://localhost:4000/api/pet";

  try {
    // Initial state
    console.log("\nðŸ“Š Getting initial state...");
    const stateRes = await fetch(`${baseUrl}/state`);
    const initialState = await stateRes.json();
    console.log("Initial state:", initialState);

    // Test positive interactions
    console.log("\nðŸ’š Testing Positive Interactions ðŸ’š");
    for (const test of positiveSpeeches) {
      console.log(`\nðŸ—£ Testing: ${test.description}`);
      const res = await fetch(`${baseUrl}/interact`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ speech: test.speech }),
      });
      const result = await res.json();
      console.log("Response:", result);
      console.log("Mood Impact:", result.analysis.moodImpact);
      console.log("Cat's Reaction:", result.analysis.catReaction);

      // Wait between tests
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }

    // Get intermediate state
    console.log("\nðŸ“Š Checking state after positive interactions...");
    const midStateRes = await fetch(`${baseUrl}/state`);
    const midState = await midStateRes.json();
    console.log("Current state:", midState);

    // Test negative interactions
    console.log("\nâ¤ï¸â€ðŸ©¹ Testing Negative Interactions â¤ï¸â€ðŸ©¹");
    for (const test of negativeSpeeches) {
      console.log(`\nðŸ—£ Testing: ${test.description}`);
      const res = await fetch(`${baseUrl}/interact`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ speech: test.speech }),
      });
      const result = await res.json();
      console.log("Response:", result);
      console.log("Mood Impact:", result.analysis.moodImpact);
      console.log("Cat's Reaction:", result.analysis.catReaction);

      // Wait between tests
      await new Promise((resolve) => setTimeout(resolve, 1000));
    }

    // Get final state
    console.log("\nðŸ“Š Getting final state...");
    const finalStateRes = await fetch(`${baseUrl}/state`);
    const finalState = await finalStateRes.json();
    console.log("Final state:", finalState);

    // Print summary
    console.log("\nðŸ“ Test Summary");
    console.log("Initial Health Points:", initialState.healthPoints);
    console.log("Final Health Points:", finalState.healthPoints);
    console.log(
      "Total Mood Change:",
      finalState.healthPoints - initialState.healthPoints
    );
  } catch (error) {
    console.error("âŒ Test failed:", error);
    if (error.response) {
      console.error("Response status:", error.response.status);
      console.error("Response data:", await error.response.text());
    }
  }
}

console.log("ðŸ± Starting Cat Interaction Tests ðŸ±");
testPetInteractions().then(() => {
  console.log("\nâœ… Tests completed");
});

================
File: tests/llm-test.ts
================
// backend/tests/llm-test.ts
import dotenv from "dotenv";
dotenv.config();

import { analyzePetInteraction } from "../services/llmService.js";

async function testLLM() {
  console.log("ðŸ” Testing LLM Service...");
  console.log("API Key present:", !!process.env.DEEPSEEK_API_KEY);

  const tests = [
    {
      type: "Positive",
      speech:
        "You're such a good kitty! I love you so much! You're the best cat ever!",
    },
    {
      type: "Negative",
      speech: "Bad cat! Get out of here! I don't want you around anymore!",
    },
    {
      type: "Neutral",
      speech: "Hey cat, what are you doing over there?",
    },
  ];

  for (const test of tests) {
    console.log(`\nðŸ—£ Testing ${test.type} Speech:`);
    console.log("Input:", test.speech);

    try {
      const result = await analyzePetInteraction(test.speech);
      console.log("âœ… Analysis Result:");
      console.log("Mood Impact:", result.moodImpact);
      console.log("Sentiment:", result.sentiment);
      console.log("Cat Reaction:", result.catReaction);
    } catch (error) {
      console.error("âŒ Test Failed:", error);
      if (error instanceof Error) {
        console.error("Error Details:", error.message);
        console.error("Stack:", error.stack);
      }
    }
  }
}

console.log("ðŸš€ Starting LLM Tests");
testLLM()
  .then(() => console.log("\nâœ¨ Tests Completed"))
  .catch((error) => console.error("âŒ Test Suite Failed:", error));

================
File: .gitignore
================
# Create a comprehensive .gitignore in project root
cat > .gitignore << 'EOL'
# Dependencies
/node_modules
**/node_modules

# Python
**/__pycache__
*.py[cod]
*.$py.class

# Virtual Environment - Specific paths
/backend/ml/venv
/backend/ml/venv/
/backend/ml/venv/*
**/venv
**/.venv

# Build artifacts
/dist
/build
*.egg-info

# Environment files
.env
.env.local
.env.*

# IDE
.vscode
.idea
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Project specific
/backend/uploads

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
EOL

================
File: index.ts
================
import dotenv from "dotenv";
dotenv.config();

import {connectToMongo} from "./startup/db.js";
import express from "express";
import setupRoutes from "./startup/routes.js";
import cors from "cors";

console.log(process.env.MONGO_URI);
console.log(process.env.FRONTEND_URL);
console.log(process.env.PORT);

const app = express();
const port = process.env.PORT || 4000;

// Enable CORS for frontend
app.use(
  cors({
    origin: process.env.FRONTEND_URL,
  })
);

// Connect to MongoDB
// connectToMongo();
app.use(express.json());

// Setup Routes
setupRoutes(app);

// Default Route
app.get("/", (req, res) => {
  res.send(`Hello, World!`);
});

// Start the Server
app.listen(port, () => {
  connectToMongo();
  console.log(`Server is running on http://localhost:${port}`);
});

================
File: package.json
================
{
  "name": "backend",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1",
    "dev": "nodemon --watch . --ext ts --exec 'tsx --require dotenv/config index.ts'",
    "test:llm": "tsx tests/llm-test.ts"
  },
  "author": "",
  "license": "ISC",
  "description": "",
  "type": "module",
  "dependencies": {
    "@paralleldrive/cuid2": "^2.2.2",
    "bcryptjs": "^2.4.3",
    "cloudinary": "^2.5.1",
    "cookie-parser": "^1.4.7",
    "cors": "^2.8.5",
    "dotenv": "^16.4.7",
    "express": "^4.21.0",
    "express-session": "^1.18.1",
    "joi": "^17.13.3",
    "jsonwebtoken": "^9.0.2",
    "lodash": "^4.17.21",
    "mongodb": "^6.9.0",
    "multer": "^1.4.5-lts.1",
    "openai": "^4.85.1"
  },
  "devDependencies": {
    "@types/express": "^5.0.0",
    "@types/jsonwebtoken": "^9.0.7",
    "@types/multer": "^1.4.7",
    "@types/lodash": "^4.17.13",
    "@types/mongoose": "^5.11.97",
    "@types/node": "^22.10.1",
    "node-fetch": "^3.3.2",
    "nodemon": "^3.1.9",
    "ts-node": "^10.9.2",
    "tsx": "^4.19.2",
    "typescript": "^5.7.3"
  }
}

================
File: .gitignore
================
# Create/modify .gitignore in project root
cat > .gitignore << 'EOL'
# Dependencies
/node_modules
**/node_modules
node_modules/

# Build outputs
/dist
/build
/.next
/out

# Environment files
.env
.env.local
.env.*
!.env.example

# IDE
/.idea
/.vscode
*.swp
*.swo

# Debug logs
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# Testing
/coverage

# Production
/build

# Misc
.DS_Store
*.pem
Thumbs.db

# Python
**/__pycache__
*.py[cod]
*.$py.class
/backend/ml/venv
**/venv
**/.venv

# Project specific
/uploads

# TypeScript
*.tsbuildinfo
next-env.d.ts
EOL

================
File: .repomixignore
================
node_modules

================
File: babel.config.js
================
module.exports = function (api) {
  api.cache(true);
  return {
    presets: ["babel-preset-expo"],
    plugins: [
      "expo-router/babel",
      "react-native-reanimated/plugin",
      [
        "module-resolver",
        {
          alias: {
            "@": "./src",
          },
          extensions: [
            ".ios.ts",
            ".android.ts",
            ".ts",
            ".ios.tsx",
            ".android.tsx",
            ".tsx",
            ".jsx",
            ".js",
            ".json",
          ],
        },
      ],
    ],
  };
};

================
File: package.json
================
{
  "dependencies": {
    "@tensorflow/tfjs-tflite": "^0.0.1-alpha.10",
    "expo-asset": "^11.0.3",
    "express": "^4.21.2",
    "multer": "^1.4.5-lts.1"
  },
  "devDependencies": {
    "@types/express": "^5.0.0",
    "@types/multer": "^1.4.12",
    "@types/node": "^22.13.4",
    "typescript": "^4.5.4"
  }
}

================
File: README.md
================
1. Copy .env to the server folder
2. Run `python -m venv venv`
3. Run `source venv/bin/activate`
4. Run `pip install -r requirements.txt`

================
File: setup.sh
================
#!/bin/bash

# Install Node.js dependencies
npm install

# Setup backend
cd backend
npm install

# Setup ML environment
cd ml
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cd ../..

echo "Setup complete!"

================
File: tsconfig.json
================
{
  "compilerOptions": {},
  "extends": "expo/tsconfig.base"
}
